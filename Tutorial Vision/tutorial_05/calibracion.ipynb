{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78c0b7ed-06a6-44e8-9579-bb9e6dc77339",
   "metadata": {},
   "source": [
    "# Tutorial 05 - Calibración de cámara\n",
    "\n",
    "Agenda:\n",
    "\n",
    "- Checkerboard\n",
    "- Modelos de Cámara \n",
    "  - distancia focal\n",
    "  - pinhole\n",
    "  - modelo de camara con distorsión \n",
    "- Calibración Monocular\n",
    "- Rectificación\n",
    "- Estimación de pose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d5148c-01f2-473b-88b4-784ca3f2a1d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b24a70d8-535b-4da1-852b-f54f313b19a5",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Este tutorial se puede ejecutar local en Jupyter lab o utilizar Google Colab.\n",
    "\n",
    "## En Google Colab \n",
    "Este tutorial se provee junto con archivos de recursos dentro de un archivo \".zip\".\n",
    "En caso de ejecutar en Google Colab hay que:\n",
    "\n",
    "1. Descomprimir el zip en algún lado\n",
    "2. Subir el contenido del zip a Google Drive en alguna carpeta (por ejemplo `udesa/I308/tutoriales/tutorial_X`)\n",
    "3. Abrir este notebook .ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087590a1-58c6-4d2f-9abe-89cbcd19ba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# TODO: establecer el path en caso de trabajar con Colab\n",
    "DRIVE_DIR = \"udesa/I308/tutoriales/tutorial_05\"\n",
    "\n",
    "# detecta si estamos corriendo en Google Colab\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  COLAB = True\n",
    "except:\n",
    "  COLAB = False\n",
    "\n",
    "if COLAB:\n",
    "    # monta Google Drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    base_path = \"/content/drive/MyDrive/\"\n",
    "    path = os.path.join(base_path, DRIVE_DIR)\n",
    "    \n",
    "    %cd {path}\n",
    "    sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78005aff-a117-41a5-8a2c-e9a6239a19fe",
   "metadata": {
    "id": "Fqj_ukuCpkvq"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd532e5-d6ee-4df4-a4db-ecb7cef6ed2a",
   "metadata": {
    "id": "SI0ftPemfV0M"
   },
   "outputs": [],
   "source": [
    "# instalamos el paquete de utilidades\n",
    "!pip install -qq git+https://github.com/udesa-vision/i308-utils.git\n",
    "\n",
    "from i308_utils import imshow, show_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f8f83e-34eb-4a23-8271-a95177c40c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# descarga dataset de calibración monocular\n",
    "from setup import download_calib_dataset\n",
    "\n",
    "download_calib_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c02e5c-5b7d-4bbf-ae49-9a5e87985d9c",
   "metadata": {},
   "source": [
    "# Modelos De Cámara\n",
    "\n",
    "## Para qué queremos tener un modelo de cámara?\n",
    "\n",
    "- Si tenemos un modelo matemático de la cámara, podemos describir\n",
    "la relación que existe entre las coordenadas de un punto en 3d en el mundo \n",
    "y su proyección en el plano de la imagen.\n",
    "\n",
    "- Calibrar una cámara es fundamental para obtener mediciones precisas y realizar correcciones en las imágenes capturadas, por ejemplo corregir los errores introducidos por distorsiones de la lente.\n",
    "\n",
    "- El modelo de cámara nos permite conocer la relación exacta entre las coordenadas 3D del mundo real y sus proyecciones 2D en la imagen.\n",
    "\n",
    "- Aplicaciones 3D\n",
    "\n",
    "  - Realidad aumentada: podemos colocar objetos virtuales en imágenes en posiciones y orientaciones precisas.\n",
    "  - Reconstrucción 3D: podremos reconstruir escenas o objetos en 3D a partir de imágenes.\n",
    "  - Medir distancias y tamaños de manera precisa\n",
    "  - Tracking de objetos.\n",
    "  - [VPS](https://geospatialworld.net/prime/business-and-industry-trends/what-is-visual-positioning-system-vps/)\n",
    "\n",
    "## Qué modelos de cámara vamos a usar hoy?\n",
    "- modelo sólo usando distancia focal\n",
    "- modelo pinhole\n",
    "- modelo de camara con distorsión\n",
    "\n",
    "\n",
    "Los dos primeros modelos vamos a calibrarlos \"a mano\", con código propio.\n",
    "Para entender de manera didáctica cómo podría resolverse.\n",
    "Para el último modelo utilizaremos OpenCV.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847ab870-629d-4752-b1f9-e1292582fda5",
   "metadata": {},
   "source": [
    "# Checkerboard\n",
    "\n",
    "¿Qué es un Checkerboard?\n",
    "\n",
    "Un checkerboard es un patrón de calibración que tiene características que facilitan su detección mediante computer vision.\n",
    "\n",
    "- Se compone por casillas en forma de mosaico, y va alternando blanco y negro como un tablero de ajedrez. \n",
    "\n",
    "- Este tipo de patrones podemos armarlos muy facilmente mediante OpenCV de manera sintética, o bien utilizar un [sitio web como este](https://markhedleyjones.com/projects/calibration-checkerboard-collection) para generarlo.\n",
    "De esa manera si queremos podemos controlar el tamaño de cada celda.\n",
    "\n",
    "- Para trabajar con este tipo de patrones los algoritmos van a asumir que todos los corners están en un plano, por lo que es conveniente pegar la hoja sobre una superficie plana y rígida.\n",
    "\n",
    "- Para evitar ambigüedades es conveniente que utilicemos una cantidad par por una cantidad impar de celdas, o vice-versa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049e0d6d-0e76-4cb3-9656-9bc744a45b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "board_image = cv2.imread(\"res/checkerboard.jpg\", 0)\n",
    "imshow(\n",
    "    board_image,\n",
    "    title=\"checkerboard 6x8\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c72c18-bbaf-4df9-9557-63cc61fd1542",
   "metadata": {},
   "source": [
    "## Detectando un checkerboard\n",
    "\n",
    "para encontrar los corners del checkerboard podemos usar la función de OpenCV [cv2.findChessboardCorners](https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga93efa9b0aa890de240ca32b11253dd4a)\n",
    "\n",
    "Esta función recibe:\n",
    "- la imagen donde buscar (en grayscale)\n",
    "- el checkerboard (un par 6x8 en este caso)\n",
    "\n",
    "y devuelve el par: (found, corners)\n",
    "\n",
    "- found: True o False, si lo encontró o no\n",
    "- corners: un array con las posiciones de los corners encontrados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b093ca2-7885-4a4d-926b-0ef0741a9e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the chess board corners\n",
    "\n",
    "checkerboard = (6, 8)\n",
    "\n",
    "found, corners = cv2.findChessboardCorners(\n",
    "    board_image,\n",
    "    checkerboard,\n",
    "    cv2.CALIB_CB_ADAPTIVE_THRESH + cv2.CALIB_CB_FAST_CHECK + cv2.CALIB_CB_NORMALIZE_IMAGE\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b277247b-f903-4b6a-8f05-8d7797717c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fe3b72-5257-4ea5-bec0-cd5cfdfc79a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obs: los corners retornados vienen en un array de (Nx1x2),\n",
    "# por eso muchas veces es necesario reshapearlos a (Nx2)\n",
    "corners.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c96b1d-f649-4bc5-87f7-6b829185ea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corners.reshape(-1, 2)[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7204d2-2140-49b3-8dc4-653bb5e765a8",
   "metadata": {},
   "source": [
    "## Mejorando precisión de los corners encontrados\n",
    "\n",
    "Si encontramos el checkerboard, entonces OpenCV nos da la funcion [cv2.cornerSubPix](https://docs.opencv.org/4.x/dd/d1a/group__imgproc__feature.html#ga354e0d7c86d0d9da75de9b9701a9a87e), donde implementa un algoritmo iterativo para mejorar la localización de los corners encontrados a nivel sub-pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0685c3d-b184-4089-9d32-fc133961054f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontramos los corners\n",
    "# refinemos las coordenadas encontradas\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "\n",
    "# refining pixel coordinates for given 2d points.\n",
    "corners = cv2.cornerSubPix(\n",
    "    board_image, \n",
    "    corners, \n",
    "    (11, 11), \n",
    "    (-1, -1), \n",
    "    criteria\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fe094d-8515-405e-9aa9-0eda4aa455a9",
   "metadata": {},
   "source": [
    "En general vamos a querer ejecutar ambas funciones en secuencia: \n",
    "\n",
    "1. cv2.findChessboardCorners\n",
    "2. cv2.cornerSubPix\n",
    "\n",
    "Para simplificar nuestro código podemos armar una función que realice ambas invocaciones.\n",
    "O bien usar la función `detect_board` que se provee en el modulo `calib.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b11af66-c9bb-4a67-9cf0-5e5a95e3dc62",
   "metadata": {},
   "source": [
    "## Graficando un checkerboard\n",
    "\n",
    "1. OpenCV provee la función [cv2.drawChessboardCorners](https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga6a10b0bb120c4907e5eabbcd22319022) para graficar los corners detectados.\n",
    "\n",
    "2. También se provee la función `draw_checkerboard` dentro del modulo `calib.py`, que hace una tarea similar,\n",
    "pero permite controlar el thickness y tamaño los circulos en el dibujo (útil para imagenes con mucha resolución)\n",
    "ya que aparentemente con drawChessboardCorners no es posible tocar esos parámetros.\n",
    "\n",
    "Ejemplo:\n",
    "```python\n",
    "\n",
    "import calib\n",
    "\n",
    "found, corners = calib.detect_board(checkerboard, image)\n",
    "\n",
    "````\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ba38a8-a697-4a6f-8002-46eee59ecd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import calib\n",
    "\n",
    "show_board = cv2.cvtColor(board_image, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "# dibujo los corners usando la función drawChessboardCorners de OpenCV\n",
    "show_board_cv = cv2.drawChessboardCorners(\n",
    "    show_board.copy(), \n",
    "    checkerboard, \n",
    "    corners, \n",
    "    found\n",
    ")\n",
    "\n",
    "# dibujo los corners usando la función drawChessboardCorners de OpenCV\n",
    "show_board_mine = calib.draw_checkerboard(\n",
    "    show_board.copy(),\n",
    "    checkerboard,\n",
    "    corners,\n",
    "    found\n",
    ")\n",
    "\n",
    "show_images([\n",
    "    #board_image,\n",
    "    show_board_cv,\n",
    "    show_board_mine\n",
    "], [\"drawChessboardCorners (CV)\", \"draw_checkerboard (Custom)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620b40ba-6972-4c49-bf58-4b3bf85f92b5",
   "metadata": {},
   "source": [
    "Observemos que como OpenCV sabe el layout del checkerboard,\n",
    "los corners devueltos son unívocos y están ordenados.\n",
    "\n",
    "Entonces puedo acceder por índice al array resultante, y se exactamente cuál es \n",
    "ese corner.\n",
    "\n",
    "Por ejemplo que los boundings de los corners estarán en los indices:\n",
    "- $0$,\n",
    "- $checkerboard_w - 1$,\n",
    "- $checkerboard_h * (checkerboard_h - 1)$ y en\n",
    "- $(checkerboard_w * checkerboard_h) - 1$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c28137-431f-4bfc-b1e6-e5c003e0bf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "ch_w, ch_h = checkerboard\n",
    "\n",
    "pts = corners.reshape(-1, 2)\n",
    "\n",
    "boundings = np.array([\n",
    "    pts[0],\n",
    "    pts[ch_w - 1],\n",
    "    pts[(ch_w * ch_h) - 1],\n",
    "    pts[(ch_w * (ch_h - 1))],\n",
    "])\n",
    "_, ax = imshow(board_image, show=False)\n",
    "ax.scatter(boundings[:, 0], boundings[:, 1], marker='x', color='r')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ebba21-4637-40dc-b15a-333c011da70d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e580b475-cc19-4d9d-abac-5f881a664654",
   "metadata": {},
   "source": [
    "# Modelo de cámara \"Distancia Focal\"\n",
    "\n",
    "En su versión más simple, nuestro modelo de cámara es **un número: la distancia focal**.\n",
    "\n",
    "\n",
    "Si asumimos que:\n",
    "- nuestra cámara se comporta como pinhole, no hay distorsiones causadas por lentes.\n",
    "- nos mantenemos siempre centrados al checkerboard de manera perpendicular (en la direccion del eje óptico)\n",
    "\n",
    "=> podemos usar similaridad de triángulos para estimar f.\n",
    "\n",
    "Cómo?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58a1930-d7bf-4596-b432-e41659e0e76a",
   "metadata": {},
   "source": [
    "## Estimando Distancia Focal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b884c23f-d98d-4310-ad58-3664bf2d40db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Medimos nuestro checkerboard para verificar el tamaño de cada casilla\n",
    "# En este caso el tamaño de cada casilla es de 24.2 mm\n",
    "\n",
    "square_size_mm = 24.2\n",
    "# ancho en casillas x alto en casillas\n",
    "checkerboard = (10, 7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20f0bc3-454b-41bd-a586-ed1a872baa34",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://github.com/udesa-vision/i308-resources/blob/main/tutoriales/tutorial_05/measure.jpg?raw=true\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87909475-4c89-47d8-8808-d9bb636a4af4",
   "metadata": {},
   "source": [
    "OBS: como todas las casillas son del mismo tamaño, podemos medir varias y dividir por la cantidad, y obtendremos menos error de medición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fdcf01-f25e-48e2-8e67-31ad540bc898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. usamos una cinta métrica y nos alejamos una distancia conocida, para capturar una imagen del checkerboard.\n",
    "img = cv2.imread(\"res/images_measured/dist_100cm.jpg\", 0)\n",
    "distance_mm = 1000\n",
    "\n",
    "# img = cv2.imread(\"recursos/images_measured/dist_200cm.jpg\", 0)\n",
    "# distance_mm = 2000\n",
    "\n",
    "# img = cv2.imread(\"recursos/images_measured/dist_40cm.jpg\", 0)\n",
    "# distance_mm = 400\n",
    "\n",
    "\n",
    "\n",
    "distance_m = \"{:.2f}\".format(distance_mm / 1000)\n",
    "imshow(img, title=f'distancia conocida={distance_m}m', figsize=(10, 6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e177772a-adfd-4e75-9010-b78865eada56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Ahora tomemos imagenes a distancia desconocida:\n",
    "# En realidad cada archivo tiene encodada la distancia a la que fue tomada la foto\n",
    "# pero supongamos que estamos en \"runtime\" donde no conocemos la distancia.\n",
    "\n",
    "\n",
    "img1 = cv2.imread(\"res/images_measured/dist_40cm.jpg\", 0)\n",
    "img2 = cv2.imread(\"res/images_measured/dist_70cm.jpg\", 0)\n",
    "img3 = cv2.imread(\"res/images_measured/dist_100cm.jpg\", 0)\n",
    "img4 = cv2.imread(\"res/images_measured/dist_150cm.jpg\", 0)\n",
    "img5 = cv2.imread(\"res/images_measured/dist_200cm.jpg\", 0)\n",
    "\n",
    "\n",
    "show_images([\n",
    "    img,\n",
    "    img1,\n",
    "    img4\n",
    "], [\n",
    "    f\"checkerboard a {distance_m}m\",\n",
    "    \"checkerboard a ?\",\n",
    "    \"checkerboard a ?\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0454afcb-5c41-48bf-894e-6e62b5dbb796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cómo podemos estimar la distancia de la cámara a los checkerboards?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebff7eb-b757-49a6-80ca-0b69137f9280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import misc\n",
    "\n",
    "misc.plot_simplest_camera_model(\n",
    "    obj_x = distance_mm,\n",
    "    objs_x_unk=[400]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e989a586-00b3-46f0-a81b-afbaf0d42471",
   "metadata": {},
   "source": [
    "Por similitud de triángulos tenemos:\n",
    "\n",
    "$\\displaystyle \\frac{f}{p} = \\frac{d}{w} $ \n",
    "\n",
    "En donde:\n",
    "- $ w $ [mm] es el ancho del objeto en el mundo\n",
    "- $ p $ [mm] es la distancia al objeto\n",
    "- $ p $ [px] es el ancho del objeto en píxeles\n",
    "\n",
    "Entonces:\n",
    "\n",
    "$\\displaystyle f = \\frac{d \\times p}{w} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5395bbc7-315e-4fa9-8d62-6b8e7aa090ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58278193-9bfa-46b0-b362-88c949e3b2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. detectamos el checkerboard en la imagen conocida\n",
    "#  se provee la función detect_board que realiza ese trabajo en el modulo calib\n",
    "import calib\n",
    "\n",
    "checkerboard = (10, 7)\n",
    "found, corners = calib.detect_board(checkerboard, img)\n",
    "\n",
    "show_board = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "show_board = calib.draw_checkerboard(\n",
    "    show_board,\n",
    "    checkerboard,\n",
    "    corners,\n",
    "    found\n",
    ")\n",
    "imshow(show_board, figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12945d6-78b8-403b-94c8-27d1c8cd7ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. tomamos solamente los corners c1=(0,0) y c2=(0, 9). c3 = (7, 9)\n",
    "\n",
    "pts = corners.reshape(-1, 2)\n",
    "\n",
    "c1 = pts[0]\n",
    "c2 = pts[9]\n",
    "c3 = pts[10 * 7 - 1]\n",
    "\n",
    "_, ax = imshow(show_board, figsize=(10, 6), show=False)\n",
    "ax.scatter(c1[0], c1[1], marker='x', color='r')\n",
    "ax.scatter(c2[0], c2[1], marker='x', color='r')\n",
    "ax.scatter(c3[0], c3[1], marker='x', color='r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe12462d-d61a-4257-a071-1b4de47c711e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83359864-260b-43b2-93da-49afdc289fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. tenemos las posiciones en pixeles, podemos obtener el ancho (y el alto) en píxeles del checkerboard\n",
    "\n",
    "w_px = np.linalg.norm(\n",
    "    c1 - c2\n",
    ")\n",
    "h_px = np.linalg.norm(\n",
    "    c3 - c2\n",
    ")\n",
    "# w_px = np.abs(c2[0] - c1[0]) \n",
    "\n",
    "w_px, h_px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44424ab2-ab4f-4f5f-ac5e-3cb7fe35eeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. conocemos el tamaño del checkerboard en el mundo real (ancho en x)\n",
    "# calculémoslo:\n",
    "\n",
    "square_size_mm = 24.2\n",
    "object_size_mm = (np.array(checkerboard) - (1, 1)) * square_size_mm\n",
    "object_size_mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5eb880-ad67-440c-9643-962a8e58cf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d446f8aa-a1a0-4652-a70e-4f89d87ac244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Listo! tenemos: \n",
    "# - la distancia en profundidad, \n",
    "# - el tamaño en pixeles\n",
    "# - el tamaño en el mundo real, \n",
    "# => podemos calcular la distancia focal.\n",
    "\n",
    "# w_px / f = w_world / distance\n",
    "# => f = w_px * distance / w_world\n",
    "\n",
    "# fx: focal len x\n",
    "obj_w_mm, obj_h_mm = object_size_mm\n",
    "\n",
    "fx = distance_mm * w_px / obj_w_mm\n",
    "fy = distance_mm * h_px / obj_h_mm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc5b620-5083-4316-be8c-49f662face0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"distance camera [mm]:\", distance_mm)\n",
    "print(\"object world size [mm]:\", np.round(obj_w_mm, 1), np.round(obj_h_mm, 1))\n",
    "print(\"object image size [px]:\", np.round(w_px), np.round(h_px))\n",
    "print(\"focal lenght (fx, fy):\", np.round(fx, 1), np.round(fy, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf94b0b-986e-4897-a9ca-77e71732b274",
   "metadata": {},
   "source": [
    "## Estimando distancia al objeto usando la distancia focal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd98b2b-a7c5-491c-aed0-72197a65d64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora que tenemos nuestro modelo de cámara:\n",
    "#\n",
    "# Creemos una función que dados:\n",
    "#\n",
    "#     - el tamaño del checkerboard\n",
    "#     - una imagen del checkerboard \n",
    "#     - la distancia focal\n",
    "#     - el tamaño del checkerboard en el mundo\n",
    "#     devuelve a qué distancia está el checkerboard.\n",
    "\n",
    "import calib\n",
    "\n",
    "def estimate_distance(\n",
    "    checkerboard, \n",
    "    img, \n",
    "    fx,\n",
    "    obj_width_mm,\n",
    "):\n",
    "\n",
    "    # Encuentra el checkerboard\n",
    "    found, corners = calib.detect_board(checkerboard, img)\n",
    "    if not found:\n",
    "        raise Exception(\"board not found\")\n",
    "\n",
    "    pts = corners.reshape(-1, 2)\n",
    "    ch_w, ch_h = checkerboard\n",
    "    c1 = pts[0]\n",
    "    c2 = pts[ch_w - 1]\n",
    "    c3 = pts[ch_w * ch_h - 1]\n",
    "\n",
    "    w_px = np.linalg.norm(\n",
    "        c1 - c2\n",
    "    )\n",
    "\n",
    "    #h_px = np.linalg.norm(\n",
    "    #    c3 - c2\n",
    "    #)\n",
    "\n",
    "    # Estima la distancia\n",
    "    distance_mm = obj_width_mm * fx / w_px\n",
    "\n",
    "    return distance_mm\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebd3e4d-1a0c-4cb4-9146-ce3531f4fc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [\n",
    "        img1,\n",
    "        img2,\n",
    "        img3,\n",
    "        img4,\n",
    "        img5\n",
    "    ]\n",
    "\n",
    "\n",
    "estimated_distances = [\n",
    "    # para cada imagen, estimamos la distancia\n",
    "    estimate_distance(\n",
    "        checkerboard,\n",
    "        img,\n",
    "        fx,\n",
    "        obj_width_mm=obj_w_mm,\n",
    "    )\n",
    "    for img in images\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e7696b-e566-474b-89e3-86ed898c5e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_distances = [\n",
    "    400,\n",
    "    700,\n",
    "    1000,\n",
    "    1500,\n",
    "    2000\n",
    "]\n",
    "\n",
    "for img, d_hat, d in zip(\n",
    "    images,\n",
    "    estimated_distances, \n",
    "    real_distances\n",
    "):\n",
    "    err = 100 * np.abs(d - d_hat) / d\n",
    "    title = \"real[mm]: {:.1f}, estim[mm]: {:.1f}, err rel: %{:.1f}\".format(\n",
    "        d, \n",
    "        d_hat,\n",
    "        err\n",
    "    )\n",
    "    imshow(img, title=title, figsize=(10, 6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b0190e-b997-412f-9838-0a6e27c30923",
   "metadata": {},
   "source": [
    "# Modelo de cámara Pinhole \n",
    "\n",
    "Repasemos,\n",
    "\n",
    "<img src=\"https://github.com/udesa-vision/i308-resources/blob/main/tutoriales/tutorial_05/pinhole_setup.png?raw=true\" width=\"100%\"/>\n",
    "\n",
    "$X_w$ y $X_c$ es mismo punto, pero visto desde distintos marcos de referencia.\n",
    "\n",
    "- $ X_w = (x_w, y_w, z_w)$ (en el marco de referencia del mundo).\n",
    "- $ X_c = (x_c, y_c, z_c)$ (en el marco de referencia de la cámara).\n",
    "- $ p = (u, v)$ (las coordenadas del pixel donde cae este punto en la imagen).\n",
    "\n",
    "El marco de referencia de la cámara está centrado en el pinhole y con z alineado con el eje óptico,\n",
    "que en la imagen pasa por el punto principal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4463172-480e-4d9f-b64d-ac7327ef66f1",
   "metadata": {},
   "source": [
    "\n",
    "## Parámetros Intrínsecos\n",
    "\n",
    "Los parámetros intrínsecos de la cámara son básicamente la distancia focal y el punto principal: $f_x, f_y, o_x, o_y $.\n",
    "\n",
    "Tenemos \"dos distancias focales\" $f_x$, $f_y$, para compensar pixeles no cuadrados, dadas $m_x$ y $m_y$, las densidades de píxeles por milímetro en cada eje, $f_x = f . m_x$, $f_y = f . m_y$. \n",
    "\n",
    "### Matriz de Calibración K\n",
    "\n",
    "Disponemos los parámetros intrínsecos de la cámara para formar la homografía $K$:\n",
    "\n",
    "$K = \\begin{bmatrix}\n",
    "    f_x & 0 & o_x  \\\\\n",
    "    0 & f_y & o_y  \\\\\n",
    "    0 & 0 & 1  \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Obs: $K$ es triangular superior.\n",
    "\n",
    "### Matriz Intrínseca Mint\n",
    "\n",
    "Es una matriz $M_{int}$ en $(3 \\times 4)$ que se arma concatenando K con un vector de ceros:\n",
    "\n",
    "$M_{int} = \\begin{bmatrix}\n",
    "    f_x & 0 & o_x & 0 \\\\\n",
    "    0 & f_y & o_y & 0 \\\\\n",
    "    0 & 0 & 1 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "    K | 0 \n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Esta matriz toma un punto 3d un punto en el marco de referencia de la cámara, expresado en coords homogeneas, y lo lleva de a un píxel en la imagen en coordenadas homogéneas.\n",
    "\n",
    "$ \\widetilde{p} = M_{int} . \\widetilde{X_c} $\n",
    "\n",
    "## Parámetros Extrínsecos\n",
    "\n",
    "Los parámetros extrínsecos son:\n",
    "\n",
    "- la traslación $t$, y\n",
    "- la rotación $R$\n",
    "\n",
    "de la cámara con respecto al marco de referencia del mundo.\n",
    "\n",
    "$R = \\begin{bmatrix}\n",
    "    r_{11} & r_{12} & r_{13} \\\\\n",
    "    r_{21} & r_{22} & r_{23} \\\\\n",
    "    r_{31} & r_{32} & r_{33} \\\\\n",
    "\\end{bmatrix}\n",
    ";\n",
    "t = \\begin{bmatrix}\n",
    "    t_{x} \\\\\n",
    "    t_{y} \\\\\n",
    "    t_{z} \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "### Rotación\n",
    "- $R$ es una matriz de Ortonormal.\n",
    "- Cada fila de $R$ representa la dirección de los versores de la cámara en el marco de referencia del mundo.\n",
    "\n",
    "### Traslación\n",
    "Este es un vector de tres elementos que indica la distancia de la cámara en las direcciones x, y, y z del sistema de coordenadas del mundo.\n",
    "\n",
    "- $ t = -R . c_w $\n",
    "\n",
    "### Matriz extrínseca Mext\n",
    "\n",
    "$M_{ext}$ en $(4 \\times 4)$ que se arma con los parámetros extrínsecos:\n",
    "\n",
    "$M_{ext} = \\begin{bmatrix}\n",
    "    r_{11} & r_{12} & r_{13} & t_x\\\\\n",
    "    r_{21} & r_{22} & r_{23} & t_y\\\\\n",
    "    r_{31} & r_{32} & r_{33} & t_z \\\\\n",
    "    0 & 0 & 0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "    R & t \\\\\n",
    "    0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Esta matriz toma un punto 3d en el marco de referencia del mundo, expresado en coords homogeneas, y lo lleva a un punto en 3d en el marco de referencia de la cámara.\n",
    "\n",
    "$ \\widetilde{X_c} = M_{ext} . \\widetilde{X_w} $\n",
    "\n",
    "## Matriz de Proyección P\n",
    "\n",
    "La matriz de Proyección $ P $, que se puede armar componiendo las matrices intrínseca y extrínseca.\n",
    "\n",
    "$P = M_{int} . M_{ext} $\n",
    "\n",
    "$\n",
    "P = \\begin{bmatrix}\n",
    "    p_{11} & p_{12} & p_{13} & p_{14} \\\\\n",
    "    p_{21} & p_{22} & p_{23} & p_{24} \\\\\n",
    "    p_{31} & p_{32} & p_{33} & p_{34} \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Esta matriz $P$ lleva un puntos desde las coordenadas del mundo 3d a píxeles en la imagen 2d.\n",
    "\n",
    "$ \\widetilde{p} = P . \\widetilde{X_w} $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513e936d-eede-4e58-87cc-5c15fba355d5",
   "metadata": {},
   "source": [
    "## Estimando modelo Pinhole con un objeto 3D conocido\n",
    "\n",
    "Necesitamos un objeto 3d con medidas conocidas.\n",
    "\n",
    "Vamos a calibrar la cámara usando la mesita de luz:\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "  <img src=\"https://github.com/udesa-vision/i308-resources/blob/main/tutoriales/tutorial_05/night_table/measuring_night_table.jpg?raw=true\" alt=\"midiento mesita de luz\" style=\"width: 50%; margin-right: 10px; object-fit: cover;\">\n",
    "  <img src=\"https://github.com/udesa-vision/i308-resources/blob/main/tutoriales/tutorial_05/night_table/measuring_night_table_model.jpeg?raw=true\" alt=\"plano 3D\" style=\"width: 50%; object-fit: cover;\">\n",
    "</div>\n",
    "\n",
    "1. Usando una cinta métrica, mido las dimesiones de la mesita de luz.\n",
    "2. Con eso puedo armar un plano 3d de la mesita\n",
    "3. Si asumo que el $(0, 0)$ es el vértice superior izquierdo de atrás, puedo saber donde está en 3d cada vértice (en coordenadas del mundo).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d919e82a-5596-4f95-9162-d7a4798e6b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coloco los puntos conocidos en 3d del objeto en un array.\n",
    "\n",
    "obj_points = np.array([\n",
    "    \n",
    "    # (0, 0, 0),\n",
    "    (45, 0, 0),\n",
    "    (45, 0, -40),\n",
    "    # (45, 57.5, -40),\n",
    "    (0, 57.5, -40),\n",
    "    (0, 57.5, 0),\n",
    "    (0, 0, -40),\n",
    "\n",
    "    # (45, 23.75, -40),\n",
    "    (15, 23.75 + 7, -40),\n",
    "    (45 - 15, 23.75 + 7, -40),\n",
    "\n",
    "    # (22.5, 23.75, -40 + 3),\n",
    "    (0, 23.75, -40 + 3),\n",
    "    \n",
    "])\n",
    "\n",
    "# usando GIMP busco las correspondencias a mano\n",
    "\n",
    "img_points = np.array([\n",
    "    # (701, 182),\n",
    "    (1163, 76),\n",
    "    (1507, 181),\n",
    "    # (1424, 805),\n",
    "    (996, 1017),\n",
    "    (795, 746),\n",
    "    (942, 346),\n",
    "\n",
    "    # (1468, 478),\n",
    "    (1148, 685),\n",
    "    (1314, 617),\n",
    "\n",
    "    # (1213, 546),\n",
    "    (948, 651),\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75309c42-057f-4272-bed1-cc8f975f0a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"res/night_table/night_table_1.jpg\", 0)\n",
    "\n",
    "_, ax = imshow(img, show=False, figsize=(10, 6))\n",
    "ax.scatter(img_points[:, 0], img_points[:, 1], marker='+', color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501cc451-980d-46ea-9b75-d05cbd2adb3b",
   "metadata": {},
   "source": [
    "Usando pares de correspondencias, armo un sistema de ecuaciones, que expreso de manera matricial.\n",
    "Muy parecido a lo que hicimos antes para estimar la H.\n",
    "La diferencia ahora es que $P$ es de $(3 \\times 4)$.\n",
    "\n",
    "$ \\widetilde{p} = P . \\widetilde{X_w} $\n",
    "\n",
    "- P tiene DoF=11.\n",
    "- Cada correspondencia aporta 2 ecuaciones.\n",
    "- Cuántos pares de correspondencias necesitamos?\n",
    "\n",
    "Esta vez resolvemos usando cuadrados mínimos, para eso aplicamos SVD y luego tomamos el autovector correspondiente al menor autovalor.\n",
    "\n",
    "<span style=\"color:red\">OJO: este método **NO FUNCIONA** si los puntos en el mundo son coplanares.</span> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f731c9-e2b2-43d8-bd64-5414b253949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_projection_matrix(world_points, image_points):\n",
    "    \"\"\"\n",
    "        Estimamos la matrix de proyección P usando pares de correspondencias\n",
    "        entre puntos en el mundo y puntos en la imagen.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Construye el sistema de ecuaciones de forma matricial,\n",
    "    # como la matriz A\n",
    "    A = []\n",
    "    for i in range(len(world_points)):\n",
    "        x_w, y_w, z_w = world_points[i]\n",
    "        u_i, v_i = image_points[i]\n",
    "        \n",
    "        A.append([x_w, y_w, z_w, 1,   0,   0,   0, 0, -u_i * x_w, -u_i * y_w, -u_i * z_w, -u_i ])\n",
    "        A.append([  0,   0,   0, 0, x_w, y_w, z_w, 1, -v_i * x_w, -v_i * y_w, -v_i * z_w, -v_i ])\n",
    "\n",
    "    A = np.array(A)\n",
    "    \n",
    "    # Usamos SVD para resolver el sistema\n",
    "    _, _, Vt = np.linalg.svd(A)\n",
    "    # el vector p que se corresponde con el menor auto-valor de A, \n",
    "    # es en efecto la matriz P, dispuesta como vector.\n",
    "    p = Vt[-1, :]  \n",
    "\n",
    "    # Reshape p to form the projection matrix P\n",
    "    P = p.reshape(3, 4)\n",
    "\n",
    "    # Normalizo P\n",
    "    P /= P[2, 3] \n",
    "\n",
    "    return P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f75e25f-588b-4749-88fa-d6294705bb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = estimate_projection_matrix(obj_points, img_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aa791c-579e-45c8-9f45-db40e780e076",
   "metadata": {},
   "outputs": [],
   "source": [
    "P.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad9499e-8fad-496b-b0d0-2faa0ddb9d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando la matriz P puedo proyectar puntos del mundo en la imagen:\n",
    "\n",
    "# cara lateral izquierda de la mesita\n",
    "face1 = np.array([\n",
    "    (0,  0, 0),\n",
    "    (0, 0, -40),\n",
    "    (0, 57.5, -40),\n",
    "    (0,  57.5, 0),\n",
    "])\n",
    "\n",
    "# borde superior lateral derecho de la mesita\n",
    "face2 = np.array([\n",
    "    (45,  0,   0),\n",
    "    (45,  0, -40),\n",
    "    (45,  4, -40),\n",
    "    (45,  4,   0),\n",
    "])\n",
    "\n",
    "# borde inferior lateral derecho de la mesita\n",
    "face3 = np.array([\n",
    "    (45,  23.75,   0),\n",
    "    (45,  23.75, -40),\n",
    "    (45,  57.5,  -40),\n",
    "    (45,  57.7,    0),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f15413-b681-4a95-983d-1ac7a0413eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_points(P, world_points):\n",
    "    # Convert world points to homogeneous coordinates (Nx4)\n",
    "    wph = np.hstack((world_points, np.ones((world_points.shape[0], 1))))\n",
    "    \n",
    "    # Project world points onto the image plane\n",
    "    proj_h = wph @ P.T\n",
    "    \n",
    "    # Convert to 2D image coordinates by normalizing with the third homogeneous coordinate\n",
    "    proj = proj_h[:, :2] / proj_h[:, 2].reshape(-1, 1)\n",
    "\n",
    "    return proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ec4711-6b5c-48fd-ab33-bb7b39d66805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Polygon\n",
    "def draw_face(ax, face, color):\n",
    "    ax.scatter(face[:, 0], face[:, 1], marker='+', color='g')\n",
    "    ax.add_patch(Polygon(face, alpha=0.5, color=color))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debf3b8e-81c2-4007-928e-116667bf5618",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f1_p = project_points(P, face1)\n",
    "f2_p = project_points(P, face2)\n",
    "f3_p = project_points(P, face3)\n",
    "\n",
    "_, ax = imshow(img, figsize=(10, 6), show=False)\n",
    "draw_face(ax, f1_p, color='y')\n",
    "draw_face(ax, f2_p, color='m')\n",
    "draw_face(ax, f3_p, color='c')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b98c9c-a5bd-42b9-8809-99e83b365fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obs: incluimos puntos que están ocluidos, pero aún así sabemos donde se proyectan.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91f8094-8987-4513-a02b-7d93c6e66275",
   "metadata": {},
   "source": [
    "## Descomposición de P\n",
    "\n",
    "Podemos descomponer P = $M_{int}$ . $M_{ext}$, usando la factorización QR.\n",
    "\n",
    "- El centro de camara está en el null space de P. $ P . c = 0 $\n",
    "Podemos encontrar $c$ usando SVD(P) y luego tomando el auto-vector que se corresponde con el menor autovalor.\n",
    "\n",
    "- Luego usamos que:\n",
    "\n",
    "$\n",
    "P = \\begin{bmatrix}\n",
    "    p_{11} & p_{12} & p_{13} & p_{14} \\\\\n",
    "    p_{21} & p_{22} & p_{23} & p_{24} \\\\\n",
    "    p_{31} & p_{32} & p_{33} & p_{34} \\\\\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "    K & 0 \\\\\n",
    "\\end{bmatrix}\n",
    " \\begin{bmatrix}\n",
    "    R & t \\\\\n",
    "    0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Luego:\n",
    "\n",
    "$ \n",
    "K \\begin{bmatrix}\n",
    "    R | t \\\\\n",
    "\\end{bmatrix}\n",
    "= K  \\begin{bmatrix}\n",
    "    R | -R . c \\\\\n",
    "\\end{bmatrix}\n",
    "=  \\begin{bmatrix}\n",
    "    M | -M . c \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Aplicamos despomosición RQ en M\n",
    "\n",
    "$ M = K . R $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd6f73c-f08a-45a6-8fb6-15f0686aff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def factorize_projection_matrix(P):\n",
    "    \n",
    "    # Nos aseguramos de que P sea una matrix (deberia ser de 3x4)\n",
    "    P = np.asarray(P)\n",
    "    \n",
    "    # Extraemos la parte superior izquerda de P (3x3) que se corresponde \n",
    "    # con K * R\n",
    "    M = P[:, :3]\n",
    "    \n",
    "    # Computamos el centro de cámara \n",
    "    # usando el null space de P\n",
    "    U, S, Vt = np.linalg.svd(P)\n",
    "    C = Vt[-1, :]\n",
    "    C = C / C[-1]  # normalizamos el vector homogeneo\n",
    "    \n",
    "    # Realizamos la descomposición RQ de M \n",
    "    # con eso obtenemos R y K\n",
    "    R, K = np.linalg.qr(np.linalg.inv(M))\n",
    "    \n",
    "    # Invertimos para obtener la K y R correctas \n",
    "    # de la factorización RQ.\n",
    "    K = np.linalg.inv(K)\n",
    "    R = np.linalg.inv(R)\n",
    "    \n",
    "    # Se asegura de que K tiene elementos positivos en la diagonal.\n",
    "    T = np.diag(np.sign(np.diag(K)))\n",
    "    K = np.dot(K, T)\n",
    "    R = np.dot(T, R)\n",
    "    \n",
    "    # Normaliza K, dividiendo por K[2, 2]\n",
    "    K = K / K[2, 2]\n",
    "    \n",
    "    # Vector de traslación t\n",
    "    # (P[:, 3] es t que es la última columna de P)\n",
    "    t = np.dot(np.linalg.inv(K), P[:, 3])\n",
    "    \n",
    "    return K, R, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af621fc6-f790-4d22-8f6a-9d5949f6fdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "K, R, t = factorize_projection_matrix(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ede571-92f9-488a-af40-e3abdd367384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# la distancia focal de K nos dio parecida a la calibración anterior :)\n",
    "K.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576d342d-734d-4fe4-8a01-52c12d4f6385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chequeemos que R es una matriz Ortonormal :)\n",
    "R @ R.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee5302e-c569-41f9-8ec9-f2a0800a3825",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(R, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243ead37-ec28-4ccf-98c0-2d09c280767b",
   "metadata": {},
   "source": [
    "## Limitaciones del modelo Pinhole.\n",
    "\n",
    "Pinhole no modela efectos como:\n",
    "\n",
    "- Distorsión de la lente\n",
    "- Aberraciones ópticas\n",
    "- Vignetting\n",
    "- Profundidad de campo y desenfoque\n",
    "- Efectos de dispersión y reflexión\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54f5b67-36aa-4da8-b33b-1ece1f1d41f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e5b1b2e-bced-447a-942e-dd8f2225f423",
   "metadata": {},
   "source": [
    "# Modelo de cámara de OpenCV ~ Pinhole + Distorsión\n",
    "\n",
    "El modelo de cámara que emplea OpenCV, es una extensión del modelo pinhole, donde se incluye el modelado de la distorsión causada por la lente.\n",
    "\n",
    "## Calibrando una cámara con OpenCV\n",
    "\n",
    "Podemos calibrar la cámara usando la función de OpenCV [`cv2.calibrateCamera`](https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga3207604e4b1a1758aa66acb6ed5aa65d).\n",
    "\n",
    "Esta función básicamente recibe un conjunto de correspondencias mundo-imagen $3d \\leftrightarrow 2d$, y nos devuelve los parámetros cámara: (la matriz intrínseca `K` y los coeficientes de distorsion `dist_coeffs`).\n",
    "\n",
    "\n",
    "\n",
    "Ejemplo de uso:\n",
    "\n",
    "```python\n",
    "    ret, K, dist_coeffs, rvecs, tvecs = cv2.calibrateCamera(\n",
    "        world_points,\n",
    "        image_points,\n",
    "        img_shape, None, None\n",
    "    )\n",
    "```\n",
    "\n",
    "**Argumentos**\n",
    "\n",
    "- **image_points**: es una lista de puntos detectados en la imagen. Cada elemento de esta lista será un numpy array con puntos en 2d.\n",
    "\n",
    "- **world_points**: es una lista de puntos en el mundo. Cada elemento de esta lista será un numpy array con puntos en 3d.\n",
    "\n",
    "- **img_shape**: es el shape de las imagenes de entrada.\n",
    "    \n",
    "\n",
    "**Resultado**\n",
    "\n",
    "`cv2.calibrateCamera` devuelve tanto los parámetros intrínsecos y extrínsecos de la cámara como los coeficientes de distorsión, proporcionando todo lo necesario para corregir la distorsión en imágenes futuras y entender cómo la cámara se posicionó en el espacio durante la captura de las imágenes de calibración.\n",
    "\n",
    "\n",
    "- **ret**: Es el error promedio de reproyección. Este valor representa la precisión de la calibración. Cuanto menor sea el valor, mejor será la calibración. En esencia, indica la cantidad promedio de error (en píxeles) entre los puntos proyectados y los puntos detectados en las imágenes.\n",
    "  \n",
    "- **K**: la matriz de calibración\n",
    "\n",
    "$K = \\begin{bmatrix}\n",
    "    f_x & 0 & o_x  \\\\\n",
    "    0 & f_y & o_y  \\\\\n",
    "    0 & 0 & 1  \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "- **dist_coeffs**: Es un vector que contiene los coeficientes de distorsión de la lente. Estos coeficientes describen las distorsiones ópticas de la cámara, como la distorsión radial y tangencial. Para el modelo estándar de distorsión en OpenCV, este vector tiene la forma $[k_1, k_2, p_1, p_2, k_3]$, en donde:\n",
    "\n",
    "  - $k_1, k_2, k_3$ son los coeficientes de distorsión radial\n",
    "  - $p_1, p_2$ son los coeficientes de distorsión tangencial\n",
    "\n",
    "- **rvecs**: Es una lista de vectores de rotación. Cada vector de rotación representa la rotación de la cámara con respecto al sistema de coordenadas del mundo para cada imagen utilizada en la calibración. Estos vectores pueden ser convertidos a matrices de rotación si es necesario.\n",
    "\n",
    "- **tvecs**: Es una lista de vectores de traslación. Cada vector de traslación representa la posición de la cámara en el espacio 3D del mundo con respecto al sistema de coordenadas del mundo para cada imagen utilizada en la calibración.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9f57ef-fa30-4483-856e-499e5de31ac0",
   "metadata": {},
   "source": [
    "### OJO - Deshabilitar Auto Foco!\n",
    "\n",
    "Muchas cámaras digitales, pueden tener Auto-Foco, que es básicamente un motor para ajustar la posición de la lente y así enfocar correctamente el objeto de la imagen. Si la lente se mueve, cambia la distancia focal y el punto principal. Con lo cual **CON AUTOFOCO NO NOS SIRVE LA CALIBRACIÓN REALIZADA**.\n",
    "\n",
    "Otras cámaras tienen la lente fija, y por lo tanto los parámetros intrínsecos son fijos.\n",
    "\n",
    "#### En Android\n",
    "Para deshabilitar el autofoco en un smartphone Android podemos configurarlo en modo \"pro\".\n",
    "En la app de cámara hay un deslizador en la parte de abajo, que moviéndolo hacia la derecha, puede verse la opción \"Pro\". OBS: No todo smartphone cuenta con esta opción.\n",
    "\n",
    "  <img src=\"https://github.com/udesa-vision/i308-resources/blob/main/tutoriales/tutorial_05/camera_pro_mode.jpeg?raw=true\" alt=\"Android Pro Mode\" width=\"30%\">\n",
    "\n",
    "En modo Pro. Seleccionamos el botón \"MF\" para evitar el foco automático.\n",
    "Deberíamos ver otro un slider, que debemos llevar al infinito.\n",
    "\n",
    "La calibración servirá mientras no toquemos este setting.\n",
    "  \n",
    "\n",
    "#### En cámara LogiTech\n",
    "\n",
    "Para el caso de la cámara logitech, se puede utilizar el [software logitune](https://www.logitech.com/en-us/video-collaboration/software/logi-tune-software.html?srsltid=AfmBOopFp5qn45SXHpnh6D6kDtT8dglM0V2taHPdgyJDcvh7Y16ziK4d) que permite desactivar el autofoco de la cámara\n",
    "\n",
    "  <img src=\"https://github.com/udesa-vision/i308-resources/blob/main/tutoriales/tutorial_05/logi_tune.png?raw=true\" alt=\"Logi Tune\" width=\"30%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7530110-a58f-4744-be86-93dee8425c7f",
   "metadata": {},
   "source": [
    "## Procedimiento de calibración\n",
    "\n",
    "OpenCV utiliza para calibrar el método de [Zhang](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr98-71.pdf).\n",
    "\n",
    "\n",
    "1. Preparación: Utilizamos un checkerboard, de medidas conocidas. \n",
    "\n",
    "2. Capturamos Imágenes: Se toman múltiples fotos del patrón desde diferentes ángulos y distancias con la cámara que se quiere calibrar (minimo 10, idealmente 20 o más). Es importante que el patrón cubra diferentes partes del campo de visión de la cámara en cada imagen.\n",
    "\n",
    "3. Detectamos los corners de cada imagen. Mapeamos cada corner a un punto conocido del patrón. Para esto podemos usar las funciones de OpenCV **`cv2.findChessboardCorners`** y **`cv2.cornerSubPix`**.\n",
    "\n",
    "4. Usamos la función de OpenCV [**`cv2.calibrateCamera`**](https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga3207604e4b1a1758aa66acb6ed5aa65d) que calcula los parámetros intrínsecos de la cámara (como la distancia focal y el punto principal) y los parámetros de distorsión (que corrigen las imperfecciones del lente). También se calculan los parámetros extrínsecos (posición y orientación de la cámara respecto al patrón) para cada imagen.\n",
    "\n",
    "OpenCV optimizará los parámetros calculados para minimizar el error entre los puntos proyectados según el modelo de cámara y los puntos detectados en las imágenes.\n",
    "\n",
    "<img src=\"https://github.com/udesa-vision/i308-resources/blob/main/tutoriales/tutorial_05/zhang_calibration_procedure.jpeg?raw=true\" width=\"100%\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61846859-9003-41dd-a4de-c89acf89395c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69482c75-84b1-457b-b4a2-e873cb66f07f",
   "metadata": {},
   "source": [
    "# Calibración en Notebook\n",
    "\n",
    "Utilizando OpenCV, calibrar la cámara monocular con la que se generó el dataset de calibración del directorio `images`.\n",
    "\n",
    "El checkerboard utilizado es de: 10x7, con 24.2 mm de casilla\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f83b34-9261-4192-986b-49bbf8509fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuro el checkerboard\n",
    "checkerboard = (10, 7)\n",
    "square_size_mm = 24.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01e4e8c-82d4-4088-84c3-bbb543bf2d1e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dcd463-00d2-4808-84a6-82083ceb8a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo las imágenes:\n",
    "import glob\n",
    "\n",
    "directory = \"./images/*.jpg\"\n",
    "rotation = None  # si las imagenes deben ser rotadas, defino rotation, si no dejo None\n",
    "scale = 1.0\n",
    "\n",
    "file_names = glob.glob(directory)\n",
    "\n",
    "# listo archivos en el directorio\n",
    "image_files = sorted(\n",
    "    glob.glob(directory),\n",
    "    key=lambda f: int(f.split(\"_\")[-1].split(\".\")[0])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50aa938-7c10-4c1b-be0e-471f0764112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45151beb-7880-405d-b08f-2ad5ee68989a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f4c691-2b04-430e-b077-ccf2900e5a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# leo las imagenes\n",
    "images = [\n",
    "    # Obs: si la imagen se saca con un celular, es posible\n",
    "    # que al leerla OpenCV nos la rote automáticamente\n",
    "    # dependiendo del sensor de orientación del celular al momento de tomar la foto\n",
    "    # para evitar eso podemos usar el flag cv2.IMREAD_IGNORE_ORIENTATION\n",
    "    cv2.imread(file, 0|cv2.IMREAD_IGNORE_ORIENTATION)\n",
    "    for file in image_files\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5458ecff-1751-4f86-a933-6a8d4367b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acff4e9-138b-42d9-82a3-8eca58a77dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rotation is not None:\n",
    "    print(\"rotating images...\")\n",
    "    images = [\n",
    "        cv2.rotate(img, rotation)\n",
    "        for img in images\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebd0431-2c53-445f-a621-415c50217f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = [5, 10, 15, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d2552f-d5d6-4280-b615-e8027720fe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostremos algunas de las imágenes del dataset:\n",
    "show_images([\n",
    "    images[i] for i in test_indices\n",
    "], grid=(2, 2), figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec0e8b2-5137-4738-b643-7e1bc598d0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import calib\n",
    "\n",
    "# armamos un modelo del mundo con los corners del tablero\n",
    "object_points = square_size_mm * calib.board_points(checkerboard)\n",
    "\n",
    "# miremos que pinta tienen los 3 primeros puntos:\n",
    "object_points[:, :3]\n",
    "#object_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d4e90b-ae82-4da7-ae33-88b75ad6ac7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a577e2-6ce8-4e2a-b34e-febf14474fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7008f5-90f5-48c6-8d2f-67159ecd28c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = None\n",
    "good_images = []\n",
    "image_points = []  # aquí acumularemos corners detectados en imagenes\n",
    "world_points = []  # aquí acumularemos los correspondientes puntos en el mundo\n",
    "\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "\n",
    "# iteramos, para cada archivo de imagen:\n",
    "for file, image in zip(image_files, images):\n",
    "\n",
    "    print(f\"detectando {file}...\")\n",
    "    \n",
    "    shape = image.shape[::-1]\n",
    "    if image_shape is None:\n",
    "        image_shape = shape\n",
    "    else:\n",
    "        # validemos que todas las imagenes sean del mismo tamaño\n",
    "        if image_shape != shape:\n",
    "            raise Exception(f\"se incluyeron imágenes de distintos tamaños? {shape} != {image_shape}\")\n",
    "\n",
    "    # detectamos el checkerboard\n",
    "    # print(\"finding checkerboard\")\n",
    "    w, h = image.shape[1], image.shape[0]\n",
    "    scaled = cv2.resize(image, (int(w * scale), int(h * scale)))\n",
    "    \n",
    "    found, corners = cv2.findChessboardCorners(\n",
    "        scaled,\n",
    "        checkerboard,\n",
    "        cv2.CALIB_CB_ADAPTIVE_THRESH + cv2.CALIB_CB_FAST_CHECK + cv2.CALIB_CB_NORMALIZE_IMAGE\n",
    "    )\n",
    "\n",
    "    if not found:\n",
    "        print(\"failed\")\n",
    "    else:\n",
    "\n",
    "        corners /= scale\n",
    "        # lo encontramos.\n",
    "\n",
    "        # intentamos refinar las coordenadas a precisión de sub-pixel\n",
    "        # print(\"subpix\")\n",
    "        corners = cv2.cornerSubPix(image, corners, (11, 11), (-1, -1), criteria)\n",
    "\n",
    "        # acumulamos esta imagen y sus correspondencias\n",
    "        world_points.append(object_points)\n",
    "        image_points.append(corners)\n",
    "        good_images.append(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ee3e12-1d08-4138-8715-658ca6d6ca9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"se encontraron {len(good_images)} checkerboards / {len(image_files)}\")\n",
    "\n",
    "images = good_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f16bb6-8f05-478a-83c8-2a43ae6a53cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grafiquemos algunos checkerboards para verificar que lo que encontró está ok\n",
    "\n",
    "show_images([\n",
    "    calib.draw_checkerboard(\n",
    "        cv2.cvtColor(images[i], cv2.COLOR_GRAY2BGR),\n",
    "        checkerboard,\n",
    "        image_points[i],\n",
    "        True,\n",
    "        # corner_radius=10,\n",
    "        # corner_thickness=5,\n",
    "        # line_thickness=5\n",
    "    ) for i in test_indices\n",
    "], grid=(2, 2), figsize=(10, 6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88649d2-3254-4afb-97f4-946bbad800cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibramos:\n",
    "\n",
    "ret, K, dist_coeffs, rvecs, tvecs = cv2.calibrateCamera(\n",
    "    world_points,\n",
    "    image_points,\n",
    "    image_shape, None, None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f526fc70-ceb5-4732-bff4-f206a2570e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "K.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7576131d-9dd4-473d-98cc-8e9703140b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7d94bf-cc7c-4664-bbd4-3aa4020a301f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31ac714-f604-4da9-9de4-2da35778d186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01727301-f8fb-420f-83b9-2e0e09f28c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from i308_utils import np_print\n",
    "\n",
    "# La calibración fue realizada, \n",
    "# los parámetros de cámara encontrados son:\n",
    "\n",
    "print(\"# K intrinsics:\")\n",
    "print(\"K = \", np_print(K))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"# distortion coefficients:\")\n",
    "print(\"dist_coeffs = \", np_print(dist_coeffs))\n",
    "\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"# image size:\")\n",
    "print(\"image_size = \", (w, h))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51077bd-2a08-43b6-bf14-62378f00c40d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e270439-42c7-4489-9805-73e536504c48",
   "metadata": {},
   "source": [
    "# Calibración con Tool de Calibración\n",
    "\n",
    "La propuesta es utilizar la\n",
    "[herramienta de calibración de cámaras](https://github.com/udesa-vision/i308-calib)\n",
    "\n",
    "**NOTA: esta herramienta utiliza la GUI de ventanas del sistema operativo No funcionará en Google Colab**.\n",
    "\n",
    "\n",
    "Necesitaremos un checkerboard físico con el que calibraremos. **Importante** asegurarse de que está pegado en una base rígida para evitar que se pandee.\n",
    "\n",
    "\n",
    "## Instalación:\n",
    "\n",
    "Se puede instalar via pip:\n",
    "\n",
    "```bash\n",
    "\n",
    "    pip install -qq git+https://github.com/udesa-vision/i308-calib.git\n",
    "\n",
    "```\n",
    "\n",
    "## Configurar herramienta\n",
    "\n",
    "Ejecutar el comando que inicia la herramienta de calibración monocular:\n",
    "\n",
    "```bash\n",
    "\n",
    "    calib-tool\n",
    "\n",
    "```\n",
    "\n",
    "### Argumentos\n",
    "\n",
    "- **video** str or int. \n",
    "Especifica el dispositivo de video, en linux podría ser algo del estilo `/dev/video<N>`\n",
    "\n",
    "- **resolution** str (optional)\n",
    " la resolución solicitada en el formato \"`<width>`x`<height>`\" en pixels\n",
    "\n",
    "- **checkerboard** str (optional) default=10x7\n",
    " distribución del checkerboard \"`<width>`x`<height>`\" en cuadrados\n",
    "\n",
    "- **square-size** str (optional) default=24.2\n",
    " el tamaño de cuadrado en milímetros\n",
    "\n",
    "- **config** str (optional)\n",
    "un archivo .yaml de configuración de captura\n",
    "\n",
    "- **data** str (optional) default=data\n",
    "directorio donde se guardará el dataset de calibración, capturas y los resultados de calibración\n",
    "\n",
    "\n",
    "\n",
    "### Ejemplos de comandos\n",
    "\n",
    "Calibrar la cámara 0 con los parámetros por default:\n",
    "\n",
    "```bash\n",
    "\n",
    "    calib-tool --video 0\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Calibrar con parámetros custom:\n",
    "\n",
    "```bash\n",
    "\n",
    "    calib-tool --video /dev/video3 --resolution 640x480 --checkerboard 9x6 --square-size 32.0 --data data_dir\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Calibrar con archivo de configuración de captura:\n",
    "\n",
    "```bash\n",
    "\n",
    "    calib-tool --config cfg/capture.yaml\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### Archivo de configuración de captura\n",
    "\n",
    "Se proporcionan algunos archivos de configuración.\n",
    "\n",
    "Para copiar los archivos de configuración al directorio de trabajo, ejecuta el comando:\n",
    "\n",
    "\n",
    "```bash\n",
    "\n",
    " copy-configs\n",
    "\n",
    "```\n",
    "\n",
    "Esto debería crear la carpeta `cfg/` con algunos archivos de configuración.\n",
    "\n",
    "\n",
    "Ejemplo de archivo de configuración (.yaml):\n",
    "\n",
    "```yaml\n",
    "\n",
    "    # video device, on linux might be /dev/video<N>\n",
    "    video: 0\n",
    "\n",
    "    # name: str (optional)\n",
    "    #   a name to identify the device\n",
    "    name: My Cute Camera\n",
    "\n",
    "    # resolution: str (optional)\n",
    "    #   requested resolution in the format \"<width>x<height>\" in pixels\n",
    "    resolution: 640x480\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "Podemos editar el archivo o crear uno nuevo para configurar nuestro dispositivo.\n",
    "\n",
    "\n",
    "### Sobreescribiendo parámetros de captura\n",
    "\n",
    "En caso de utilizar archivo de configuración, algunos parametros como `video` y `resolution` se pueden sobreescribir, con los argumentos de la linea de comandos:\n",
    "\n",
    "Por ejemplo, el siguiente comando va a utilizar el video 3, independientemente de lo que esté configurado en el archivo .yaml:\n",
    "\n",
    "```bash\n",
    "\n",
    "    calib-tool --video 3 --config cfg/capture.yaml \n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fa0283-bd84-45da-86f4-aa20d3c095a1",
   "metadata": {},
   "source": [
    "##  Usando la herramienta\n",
    "\n",
    "Al iniciar la herramienta vemos la captura de video.\n",
    "Realizaremos distintos comandos con teclas de la computadora (la ventana debe tener el foco).  \n",
    "\n",
    "<img src=\"https://github.com/udesa-vision/i308-resources/blob/main/tutoriales/tutorial_05/calb-tool/calib-tool-01.jpg?raw=true\" width=\"75%\"/>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Para activar / desactivar detección usar la tecla `d`.\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/udesa-vision/i308-resources/blob/main/tutoriales/tutorial_05/calb-tool/calib-tool-02.jpg?raw=true\" width=\"75%\"/>\n",
    "\n",
    "---\n",
    "\n",
    "Con la tecla `a` se pueden agregar elementos al dataset de calibración.\n",
    "Las imágenes se guardarán en el directorio de `data/calib`.  \n",
    "\n",
    "<img src=\"https://github.com/udesa-vision/i308-resources/blob/main/tutoriales/tutorial_05/calb-tool/calib-tool-03.jpg?raw=true\" width=\"75%\"/>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Vamos agregando imágenes al dataset de calibración hasta cubrir todas las regiones del sensor de la cámara.\n",
    "Es importante variar lo más posible las posiciones y la orientación del checkerboard (ej, rotado 90º, más cerca, más lejos, en diagonal y con gran variación en el eje Z), esto dará mayor estabilidad al algoritmo de calibración.\n",
    "En especial, los corners de la imagen, son los lugares de mayor distorsión y es conveniente tener ejemplos que cubran esas áreas.\n",
    "\n",
    "Capturamos al menos 10 imagenes, idealmente más de 20.\n",
    "\n",
    "<img src=\"https://github.com/udesa-vision/i308-resources/blob/main/tutoriales/tutorial_05/calb-tool/calib-tool-04.jpg?raw=true\" width=\"75%\"/>\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Una vez que tenemos una buena cantidad de ejemplos presionamos la tecla `c` para obtener los parámetros de cámara:\n",
    "\n",
    "\n",
    "```text\n",
    "\n",
    "    calibrating...\n",
    "    # Camera matrix : \n",
    "    \n",
    "    K =  np.array([\n",
    "            [  1455.282,         0.000,        946.688],\n",
    "            [     0.000,      1456.869,        566.010],\n",
    "            [     0.000,         0.000,          1.000]\n",
    "    ])\n",
    "    \n",
    "    # Distortion Coefficients : \n",
    "    \n",
    "    dist =  np.array([\n",
    "            [  0.062529,     -0.221484,       0.000169,      -0.000469,       0.180346]\n",
    "    ])\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38d3a15-a0f2-4f60-8525-786276e76db0",
   "metadata": {},
   "source": [
    "## Notas de uso\n",
    "- presionando `q` terminamos el programa\n",
    "- presionando `h` vemos el menú de ayuda con las teclas y sus correspondientes comandos.\n",
    "- si algo sale catastróficamente mal simplemente borrar el directorio `data` y comenzar de nuevo.\n",
    "- si la tool se cerró y queremos agregar nuevos ejemplos al dataset de calibración podemos cargar las imágenes del dataset con la tecla `l`\n",
    "- Cuando el algoritmo no ve el tablero, tarda más que cuando lo ve. en caso de andar lento se puede apagar la deteccion con la letra `d` centrar el tablero y volver a activar la detección.\n",
    "- presionando `s` podemos tomar capturas de objetos, que se guardarán en el directorio `data/captures`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2f00b2-df5a-46a2-9426-e5913f94edef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9473d757-8882-42d9-afe9-2b666bcc7487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ceca0c-e33d-4951-b64e-7abcae807648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdb045ce-18f3-49dd-8b54-f632d9963b98",
   "metadata": {},
   "source": [
    "# Distorsión\n",
    "\n",
    "OpenCV modela distorsión radial y tangencial.\n",
    "\n",
    "Los coeficientes de distorsión son $[k_1, k_2, p_1, p_2, k_3]$, en donde:\n",
    "\n",
    "  - $k_1, k_2, k_3$ son los coeficientes de distorsión radial\n",
    "  - $p_1, p_2$ son los coeficientes de distorsión tangencial\n",
    "\n",
    "\n",
    "**Distorsión radial**\n",
    "hace que las líneas rectas parezcan curvas, y aumenta cuanto más alejados están los puntos del centro de la imagen estemos.\n",
    "Se modela como:\n",
    "\n",
    "- $x_{distorted\\_rad} = x . (1 + k_1 . r^2 + k_2 . r^4 + k_3 . r^6 )$\n",
    "- $y_{distorted\\_rad} = y . (1 + k_1 . r^2 + k_2 . r^4 + k_3 . r^6 )$\n",
    "\n",
    "En donde:\n",
    "- $ r = \\sqrt{ x^2 + y ^ 2}$, es la distancia al centro \n",
    "\n",
    "**Distorsión tangencial** \n",
    "se produce porque la lente no está perfectamente alineada paralela al plano de la imagen. Por lo tanto, algunas áreas de la imagen pueden parecer más cercanas de lo esperado.\n",
    "Se modela como:\n",
    "\n",
    "- $x_{distorted\\_tan} = x + [2  p_1  x  y + p_2  (r^2 + 2  x^2)] $\n",
    "- $y_{distorted\\_tan} = y + [p_1  (r^2 + 2  y^2) + 2  p_2  x y] $\n",
    "\n",
    "**Corrección de distorsión** \n",
    "\n",
    "\n",
    "OpenCV primero normaliza las coordenadas para que queden relativas relativas al centro óptico.\n",
    "OpenCV aplica ambas distorsiones en una única cuenta:\n",
    "\n",
    "- $x_{distorted} = x . (1 + k_1 . r^2 + k_2 . r^4 + k_3 . r^6 ) + [2  p_1  x  y + p_2  (r^2 + 2  x^2)]  $\n",
    "- $y_{distorted} = y . (1 + k_1 . r^2 + k_2 . r^4 + k_3 . r^6 ) + [p_1  (r^2 + 2  y^2) + 2  p_2  x y] $\n",
    "\n",
    "Este modelo simula cómo se distorsiona un punto ideal y donde iría a parar en la imagen.\n",
    "Pero para des-distorsionar necesitamos la operación inversa. Como invertir esta función es muy difícil, OpenCV utiliza métodos numéricos (Ej. Newton-Rhapson) para aproximar.\n",
    "\n",
    "\n",
    "## Rectificando con OpenCV\n",
    "\n",
    "Llamamos rectificar a la remoción de distorsión.\n",
    "Ahora que tenemos el modelo de cámara con sus correspondientes coeficientes de distorsión, podemos eliminarla. \n",
    "\n",
    "OpenCV provee con dos métodos para des-distorsionar:\n",
    "\n",
    "1. usando la función [`cv2.undistort`](https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga69f2545a8b62a6b0fc2ee060dc30559d), o bien\n",
    "2. usando [`cv2.initUndistortRectifyMap`](https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga7dfb72c9cf9780a347fbe3d1c47e5d5a) y luego [`cv2.remap`](https://docs.opencv.org/4.x/da/d54/group__imgproc__transform.html#gab75ef31ce5cdfb5c44b6da5f3b908ea4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51be27df-4dff-47c6-97cd-5742e1216b86",
   "metadata": {},
   "source": [
    "## Rectificando con cv2.undistort\n",
    "\n",
    "Apliquemos cv2.undistort, para rectificar una imagen:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcb875d-e897-4fbc-b090-404fd609e04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# des-distorsionemos alguna imagen\n",
    "image_index = 14\n",
    "\n",
    "image = images[image_index]\n",
    "\n",
    "undistorted = cv2.undistort(\n",
    "    image, \n",
    "    K, \n",
    "    dist_coeffs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e177c6a2-c47e-41e0-995e-5bd7a3b121bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "axs = show_images([\n",
    "    image, \n",
    "    undistorted\n",
    "], [  \n",
    "    \"original\",\n",
    "    \"undistorted\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f040b06-af0d-4edf-a484-f8e619642701",
   "metadata": {},
   "source": [
    "## Optimal K - undistort ajustando bordes\n",
    "\n",
    "Des-distorsionar puede traernos problemas en los bordes, recortando partes de la imagen, o dejarnos porciones en negro. Para eso OpenCV provee la función [`cv2.getOptimalNewCameraMatrix`](https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga7a6c4e032c97f03ba747966e6ad862b1), la cual nos devuelve una nueva matriz de cámara y una ROI, con el área óptima en la imagen desdistorsionada.\n",
    "\n",
    "Esta función recibe el parámetro alpha, si lo seteamos en 0, va a cropear lo más posible la parte negra dejando la imagen centrada y sin bordes negros. Si lo seteamos en 1, va a mostrar toda la información de la imagen, incluso los bordes más des-distorsionados, pero puede introducir partes negras sin información, debido a la des-distorsión.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5e2c73-822a-4a32-b350-d86b3a7ca953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def undistort(image, K, dist_coeffs, alpha):\n",
    "    \n",
    "    w, h = image.shape[1], image.shape[0]\n",
    "    image_shape = (w, h)\n",
    "    \n",
    "    new_K, roi = cv2.getOptimalNewCameraMatrix(\n",
    "        K, dist_coeffs, \n",
    "        image_shape, \n",
    "        alpha, \n",
    "        image_shape\n",
    "    )\n",
    "\n",
    "    undistorted = cv2.undistort(\n",
    "        image, \n",
    "        K, \n",
    "        dist_coeffs,\n",
    "        None,\n",
    "        new_K\n",
    "    )\n",
    "\n",
    "    return undistorted, new_K, roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03645c0-5905-4c4d-a4fe-6af986badc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "undistorted_0, K_0, roi_0 = undistort(image, K, dist_coeffs, alpha=0)\n",
    "undistorted_1, K_1, roi_1 = undistort(image, K, dist_coeffs, alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed59544-ffe7-4120-adaf-2c70791b9d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images([\n",
    "    image,\n",
    "    undistorted,\n",
    "    undistorted_0,\n",
    "    undistorted_1\n",
    "], [\n",
    "    \"original\",\n",
    "    \"undistorted\",\n",
    "    \"undistorted alpha=0\",\n",
    "    \"undistorted alpha=1\",\n",
    "], grid=(2,2), figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa4d772-ed56-41d1-ad3b-4c1cde7ca29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import patches\n",
    "ret = {}\n",
    "fig, axes = show_images([\n",
    "    undistorted_0,\n",
    "    undistorted_1\n",
    "], [\n",
    "    \"undistorted alpha=0\",\n",
    "    \"undistorted alpha=1\",\n",
    "], show=False)\n",
    "\n",
    "x, y, w, h = roi_0\n",
    "axes[0].add_patch(patches.Rectangle(\n",
    "    (x, y), w, h, linewidth=1, edgecolor='r', facecolor='none'\n",
    "))\n",
    "x, y, w, h = roi_1\n",
    "axes[1].add_patch(patches.Rectangle(\n",
    "    (x, y), w, h, linewidth=1, edgecolor='r', facecolor='none'\n",
    "))\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6619a68-69fe-4464-97fc-cd9e8a3b42b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import visualize_distortions\n",
    "\n",
    "img_w, img_h = image_shape\n",
    "visualize_distortions.visualize_2d_distortions(\n",
    "    img_w, \n",
    "    img_h, \n",
    "    K, \n",
    "    dist_coeffs.reshape(-1),\n",
    "    grid_step=40\n",
    "    # grid_step=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaa14a4-8c7d-4fba-bc44-56ff52fbb2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_distortions.visualize_3d_distortions(\n",
    "    img_w, \n",
    "    img_h, \n",
    "    K, \n",
    "    dist_coeffs.reshape(-1),\n",
    "    grid_step=int(img_h / 100),\n",
    "    zlim=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02c02d2-c832-4f15-bec5-a98aef239d05",
   "metadata": {},
   "source": [
    "## Removiendo distorsión con cv2.initUndistortRectifyMap y cv2.remap\n",
    "\n",
    "Invocar a la función undistort, requiere realizar un cómputo de resultados intermedios.\n",
    "\n",
    "**Queremos des-distorsionar más rápido**\n",
    "\n",
    "OpenCV provee la función cv2.initUndistortRectifyMap que devuelve estos resultados intermedios, que luego aplicaremos con la función cv2.remap.\n",
    "\n",
    "En aplicaciones donde se necesita procesar múltiples imágenes, como en videos o secuencias de imágenes en aplicaciones de visión por computadora, calcular los mapas de transformación una sola vez con initUndistortRectifyMap y luego usar remap puede ser significativamente más eficiente.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03402a52-ed0c-4281-8b29-0a1005ecf2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso:\n",
    "\n",
    "# 1. Obtengo la nueva matriz de cámara\n",
    "alpha = 1  # Se puede usar alpha = 0, 1, o cualquier valor intermedio\n",
    "new_K, roi = cv2.getOptimalNewCameraMatrix(K, dist_coeffs, image_shape, alpha, image_shape)\n",
    "\n",
    "# 2: inicializamos los mapas de des-distorsion\n",
    "map1, map2 = cv2.initUndistortRectifyMap(K, dist_coeffs, None, new_K, image_shape, cv2.CV_32FC1)\n",
    "\n",
    "# 3: aplicamos remap para des-distorsionar la imagen\n",
    "undistorted_image_faster = cv2.remap(image, map1, map2, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "# 4: opcionalmente, podemos croppear la imagen usando el ROI devuelto en el paso 1.\n",
    "#x, y, w, h = roi\n",
    "#undistorted_image_cropped = undistorted_image[y:y+h, x:x+w]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630e4863-75b6-4faa-96ff-d4d20732644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images([\n",
    "    image,\n",
    "    undistorted_1,\n",
    "    undistorted_image_faster\n",
    "], [\n",
    "    \"original\",  \"undistorted\", \"undistorted faster\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2f69fc-fb95-44b4-934c-5c4692dfd00a",
   "metadata": {},
   "source": [
    "# Proyectando Puntos\n",
    "\n",
    "OpenCV provee la función [`cv2.projectPoints`](https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga1019495a2c8d1743ed5cc23fa0daff8c) que dados los parámetros de cámara, nos permite proyectar puntos en el mundo en la imagen.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "95814f3c-16d1-4d66-872f-e7b505e1b7e1",
   "metadata": {},
   "source": [
    "# La calibración nos había dado los parámetros intrínsecos de la cámara, \n",
    "# y los extrínsecos para cada imagen.\n",
    "\n",
    "ret, K, dist_coeffs, rvecs, tvecs = cv2.calibrateCamera(\n",
    "    world_points,\n",
    "    image_points,\n",
    "    image_shape, None, None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dd408a-0d17-40ed-a884-e5695364b6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tomemos una imagen, con sus parámetros extrínsecos\n",
    "image_index = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e64613-b38c-4a6f-ba41-104e894da6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = images[image_index]\n",
    "# corners = image_points[image_index]\n",
    "checkerboard_world_points = world_points[image_index]\n",
    "rv = rvecs[image_index]\n",
    "tv = tvecs[image_index]\n",
    "\n",
    "# Project 3D points to the 2D image plane\n",
    "projected_points, _ = cv2.projectPoints(\n",
    "    checkerboard_world_points, \n",
    "    rv, \n",
    "    tv, \n",
    "    K, \n",
    "    dist_coeffs\n",
    ")\n",
    "projected_points = projected_points.reshape(-1, 2).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac438295-9f0c-45c3-a999-2775a85fb88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = imshow(\n",
    "    image,\n",
    "    title=\"obs: son puntos proyectados. (no los detectados en la imagen)\",\n",
    "    figsize=(10, 6),\n",
    "    show=False\n",
    ")\n",
    "ax.scatter(\n",
    "    projected_points[:, 0],\n",
    "    projected_points[:, 1],\n",
    "    marker='+',\n",
    "    color='r'\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6dfe28-eeee-49c3-95fd-4fd0c5e6f1c3",
   "metadata": {},
   "source": [
    "Podemos proyectar objetos virtuales como los ejes del marco de referencia del mundo\n",
    "En este caso nuestra unidad métrica son casilleros del tablero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c0fcd8-4692-4f91-87ae-82091c144aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# axis_length = 200  # Length of the axis lines\n",
    "#axis_len_x = 217.8 \n",
    "#axis_len_y = 145.2\n",
    "#axis_len_z = -100\n",
    "axis_len_x = 9 * square_size_mm\n",
    "axis_len_y = 6 * square_size_mm\n",
    "axis_len_z = -6 * square_size_mm\n",
    "axis_points = np.array([[0, 0, 0], \n",
    "                          [axis_len_x, 0, 0], \n",
    "                          [0, axis_len_y, 0], \n",
    "                          [0, 0, axis_len_z]], dtype=np.float32)\n",
    "\n",
    "\n",
    "# Project 3D points to the 2D image plane\n",
    "axis_points, _ = cv2.projectPoints(axis_points, rv, tv, K, dist_coeffs)\n",
    "\n",
    "# Draw the axes on the image\n",
    "axis_points = axis_points.reshape(-1, 2)\n",
    "origin = tuple(axis_points[0].ravel())\n",
    "\n",
    "# show_image = cv2.undistort(img, left_cam_matrix, left_dist_coeffs)\n",
    "show_image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "def draw_line(img, pt1, pt2, color, thickness=3):\n",
    "    pt1 = (np.round(pt1[0]).astype(int), np.round(pt1[1]).astype(int))\n",
    "    pt2 = (np.round(pt2[0]).astype(int), np.round(pt2[1]).astype(int))\n",
    "    ret = cv2.line(img, pt1, pt2, color, thickness)\n",
    "    return ret\n",
    "\n",
    "draw_line(show_image, origin, axis_points[1], (0, 0, 255), 10)\n",
    "draw_line(show_image, origin, axis_points[2], (0, 255, 0), 10)\n",
    "draw_line(show_image, origin, axis_points[3], (255, 0, 0), 10)\n",
    "\n",
    "\n",
    "_, ax = imshow(\n",
    "    show_image,\n",
    "    title=\"proyectamos los ejes del mundo (con -z)\",\n",
    "    figsize=(10, 6),\n",
    "    show=False\n",
    ")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225c24d6-53b1-45b0-aad1-e225bab22fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# podemos armar una funcion conveniente para graficar ejes con Matplotlib\n",
    "\n",
    "def plot_line(ax, pt1, pt2, color, thickness=None, label=None):\n",
    "\n",
    "    if thickness is None:\n",
    "        thickness = 3\n",
    "    \n",
    "    ax.plot(\n",
    "        [pt1[0], pt2[0]], \n",
    "        [pt1[1], pt2[1]],\n",
    "        color=color,\n",
    "        linewidth=thickness,\n",
    "        label=label\n",
    "    )\n",
    "    \n",
    "def plot_axis(ax, axis, thickness=None, labels=None):\n",
    "    origin = axis[0]\n",
    "    red = (1, 0, 0)\n",
    "    green = (0, 1, 0)\n",
    "    blue = (0, 0, 1)\n",
    "    if labels is None:\n",
    "        labels = [\"x\", \"y\", \"z\"]\n",
    "    plot_line(ax, origin, axis_points[1], red, thickness, labels[0])\n",
    "    plot_line(ax, origin, axis_points[2], green, thickness, labels[1])\n",
    "    plot_line(ax, origin, axis_points[3], blue, thickness, labels[2])\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "fig, ax = imshow(image, figsize=(10, 6), show=False)\n",
    "\n",
    "plot_axis(ax, axis_points, labels=[\"x\", \"y\", \"(-z)\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0654b77-c447-4d15-81bb-ce7747bb0708",
   "metadata": {},
   "source": [
    "## Manejando proyecciones con la distorsión\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fc9366-7c88-486e-939c-9ad915464414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomemos una imagen de la calibración, con sus\n",
    "# - corners detectados en la imagen,\n",
    "# - correspondientes puntos en el mundo,\n",
    "# - y parámetros extrínsecos de la calibración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ae60ef-5559-46f2-8404-c088e0ace854",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = 22\n",
    "\n",
    "image = images[image_index]\n",
    "checkerboard_image_points = image_points[image_index]\n",
    "checkerboard_world_points = world_points[image_index]\n",
    "rv = rvecs[image_index]\n",
    "tv = tvecs[image_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560fe104-6c6b-4042-86b1-8dbf4f06793f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8b2330-5627-4747-b45f-47aa921edd39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6660f14-65c9-461a-bb7a-d9b49a8dcd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pts = checkerboard_image_points.reshape(-1, 2)\n",
    "_, ax = imshow(\n",
    "    image, \n",
    "    title=\"estos son los corners detectados\",\n",
    "    figsize=(10, 6),\n",
    "    show=False\n",
    ")\n",
    "ax.scatter(img_pts[:, 0], img_pts[:, 1], color='r', s=5, marker='+')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c7604b-89ec-438d-b07c-cf83a97cb16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# si proyectamos los puntos del checkerboard obtenemos:\n",
    "\n",
    "checkerboard_projected_points, _ = cv2.projectPoints(\n",
    "    checkerboard_world_points,\n",
    "    rv, \n",
    "    tv, \n",
    "    K, \n",
    "    dist_coeffs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f874c95e-64cc-4eed-9bdf-1980fcf3ae02",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_pts = checkerboard_projected_points.reshape(-1, 2)\n",
    "\n",
    "_, ax =imshow(\n",
    "    image, \n",
    "    \"estos son puntos del mundo proyectados (usando K, dist, rvec y tvec)\",\n",
    "    figsize=(10, 6),\n",
    "    show=False\n",
    ")\n",
    "ax.scatter(proj_pts[:, 0], proj_pts[:, 1], color='lime', s=5, marker='+')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fed3e3e-afcd-48d0-9199-a75b8c37c68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pero si quitamos distorsión en la imagen, \n",
    "# los puntos proyectados caen en posiciones incorrectas:\n",
    "\n",
    "undistorted_image = cv2.undistort(\n",
    "    image,\n",
    "    K,\n",
    "    dist_coeffs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95824086-e068-490d-9cab-b5f0995141ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = imshow(\n",
    "    undistorted_image,\n",
    "    \"imagen rectificada. los puntos proyectados no coinciden\",\n",
    "    figsize=(10, 6),\n",
    "    show=False\n",
    ")\n",
    "ax.scatter(proj_pts[:, 0], proj_pts[:, 1], color='r', s=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd18079b-6aaf-4ceb-9529-f03934a69763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feec08a1-6991-4c01-a95b-0ac3a71ff3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eso lo podemos arreglar usando la funcion cv2.undistortImagePoints,\n",
    "# que des-distorsiona coordenadas de la imagen:\n",
    "proj_pts_undistorted = cv2.undistortImagePoints(\n",
    "    proj_pts,\n",
    "    K,\n",
    "    dist_coeffs\n",
    ")\n",
    "\n",
    "\n",
    "proj_pts_undistorted = proj_pts_undistorted.reshape(-1, 2)\n",
    "_, ax = imshow(\n",
    "    undistorted_image,\n",
    "    title=\"imagen rectificada - puntos rectificados con cv2.undistortImagePoints()\",\n",
    "    figsize=(10, 6),\n",
    "    show=False\n",
    ")\n",
    "ax.scatter(\n",
    "    proj_pts_undistorted[:, 0], \n",
    "    proj_pts_undistorted[:, 1], \n",
    "    color='lime', \n",
    "    s=5\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a41d37c-29e7-4c63-861f-3adf634fdbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# o bien, si partimos de puntos en el mundo,\n",
    "# podemos remover la distorsión de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0a81b0-4aa0-48c9-8656-5e3eab083626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculamos la nueva matriz de calibración, dado alpha en [0 .. 1]\n",
    "alpha = 1.0\n",
    "new_K, roi = cv2.getOptimalNewCameraMatrix(\n",
    "    K, \n",
    "    dist_coeffs, \n",
    "    image_shape, \n",
    "    alpha,\n",
    "    image_shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ea5745-f9cc-405c-8ded-0eadbc30d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rectificamos la imagen usando la nueva matriz de camara.\n",
    "undistorted_image_optim = cv2.undistort(\n",
    "    image,\n",
    "    K,\n",
    "    dist_coeffs,\n",
    "    None,\n",
    "    new_K\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d4fcbe-5653-4de2-9f40-7127fe1641cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8695ccb5-593b-4f2a-acb3-4d15ba8e465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proyectamos los puntos. \n",
    "# OBS: usando la nueva matriz de cámara y SIN distorsión\n",
    "optim_projected, _ = cv2.projectPoints(\n",
    "    checkerboard_world_points, \n",
    "    rv, \n",
    "    tv, \n",
    "    new_K, \n",
    "    np.zeros_like(dist_coeffs)  # No distortion for undistorted image\n",
    ")\n",
    "optim_projected = optim_projected.reshape(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f081f38-ab06-46f6-a861-f059d368a039",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = imshow(\n",
    "    undistorted_image_optim,\n",
    "    title=\"imagen rectificada, puntos del mundo proyectados con new_K, dist=0\",\n",
    "    figsize=(10, 6),\n",
    "    show=False,\n",
    ")\n",
    "ax.scatter(optim_projected[:, 0], optim_projected[:, 1], color='lime', s=5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9805c60-6e71-43ac-8f73-3e20eeca45f1",
   "metadata": {},
   "source": [
    "# Pose de la cámara\n",
    "\n",
    "Calcular la pose consiste en resolver la rotación y traslación que minimiza el error de reproyección a partir de correspondencias de puntos 3D-2D.\n",
    "\n",
    "<img src=\"https://docs.opencv.org/4.x/pnp.jpg\" />\n",
    "\n",
    "## Perspectiva-por-N-Puntos (PnP)\n",
    "PnP o Perspectiva-por-N-Puntos, es un método que estima la pose de la cámara en relación a puntos conocidos (ej. los puntos de un objeto), es decir, la traslación y rotación de la cámara con respecto al objeto.\n",
    "\n",
    "OpenCV nos provee la función [`cv2.solvePnP( \"correspondencias\", \"parametros de cámara\", ...) -> pose`](https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga549c2075fac14829ff4a58bc931c033d) que implementa este método, permitiéndonos encontrar la posición y orientación de la cámara dado un conjunto de puntos 3D en el mundo y sus correspondientes proyecciones 2D en la imagen.\n",
    "\n",
    "**solvePnP flags**\n",
    "OpenCV implementa [diferentes algoritmos para estimar la pose](https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#gga357634492a94efe8858d0ce1509da869a9f589872a7f7d687dc58294e01ea33a5).\n",
    "Podemos usar los distintos métodos mediante el argumento `flags`, por default utiliza un algoritmo iterativo. El método que utilizaremos dependerá de las características de nuestro problema.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e298c81-ddcc-4347-bcdf-6a1437b5cbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de PnP\n",
    "\n",
    "image_index = 6\n",
    "\n",
    "image = images[image_index]\n",
    "checkerboard_image_points = image_points[image_index]\n",
    "checkerboard_world_points = world_points[image_index]\n",
    "rv = rvecs[image_index]\n",
    "tv = tvecs[image_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2724edc-9a3f-407d-bb32-0abea311195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(\n",
    "    image,\n",
    "    figsize=(10, 6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42efa07c-2c41-4969-a2ac-bfb3bece1ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solvePnP devuelve:\n",
    "# - exito? True si pudo estimar y False si no.\n",
    "# - rvec: la rotación de la cámara -> En formato Rodrigues\n",
    "# - tvec: la traslación de la cámara\n",
    "\n",
    "# flags=cv2.SOLVEPNP_IPPE\n",
    "# flags=cv2.SOLVEPNP_EPNP\n",
    "\n",
    "ret, rvec, tvec = cv2.solvePnP(\n",
    "    checkerboard_world_points,\n",
    "    checkerboard_image_points,\n",
    "    K,\n",
    "    dist_coeffs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab6d007-f00d-441a-b14c-81f669e950e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3c4f96-64be-48c6-8af4-08b34e9fe5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rvec, tvec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021b5da4-246a-43ac-b9be-36978a50cac0",
   "metadata": {},
   "source": [
    "## Rodrigues\n",
    "\n",
    "El vector de Rodrigues es una forma compacta de representar rotaciones en 3D mediante un vector de tres elementos. Este vector representa un eje de rotación y un ángulo, lo que permite expresar rotaciones sin necesidad de usar una matriz de rotación completa (3x3).\n",
    "\n",
    "La dirección del vector representa la dirección de rotación (eje), y la magnitud del vector representa el ángulo de rotación en radianes (alrededor del eje).\n",
    "\n",
    "Dado un vector de Rodrigues, podemos obtener una matriz de rotación R, usando la función de OpenCV [`cv2.Rodrigues`](https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga61585db663d9da06b68e70cfbf6a1eac)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab40c1b7-8daf-4a80-b803-2be2c711feeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "R, _ = cv2.Rodrigues(\n",
    "    rvec\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9662096d-0c35-4572-837d-aafe6861b18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9202f09-8b9a-43d3-b054-4d1057f0e481",
   "metadata": {},
   "outputs": [],
   "source": [
    "(R @ R.T).round(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312de353-e70f-4a83-9254-ef9466dde934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# o bien pasar de una matriz de rotación, a vectores de rodrigues usando la misma función.\n",
    "rvec, cv2.Rodrigues(R)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccff7f3-1838-44f6-b571-8f5224901825",
   "metadata": {},
   "source": [
    "## Pose en runtime ¿Cómo la usamos?\n",
    "\n",
    "Muchos problemas relacionados con cámara tienen dos fases:\n",
    "- una primera fase de calibración (~training del modelo) y\n",
    "- tiempo de ejecución o \"runtime\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d364e05-30ce-4c3a-972d-00dc0556f4b5",
   "metadata": {},
   "source": [
    "**Prerrequisitos**\n",
    "  - Ya realizamos la calibración y tenemos los parámetros de cámara `K` y `dist_coeffs`\n",
    "  - En la imagen se puede detectar un objeto conocido del cual tenemos sus coordenadas 3d\n",
    "    en el marco de referencia del mundo `obj_world_points`. \n",
    "\n",
    "**Runtime**\n",
    "  1. Detectamos el objeto en la imagen, y obtenemos `obj_image_points` que son los puntos en 2d del objeto.\n",
    "  2. Establecemos correspondencias entre los puntos $3d \\leftrightarrow 2d$.\n",
    "  3. Usando las correspondencias y los parámetros de cámara, computamos PnP para obtener la pose `R`, `t`\n",
    "\n",
    "Si sabemos la pose en runtime podemos:\n",
    "\n",
    "- transformar coordenadas de objetos en el marco de referencia del mundo al marco de referencia de la cámara\n",
    "- transformar las coordenadas de la cámara al marco de referencia del mundo\n",
    "\n",
    "entre otras cosas...\n",
    "\n",
    "**A. del mundo al marco de referencia de la cámara**\n",
    "\n",
    "4. Con la pose podemos armar la matriz extrínseca $M_{ext}$\n",
    "\n",
    "$M_{ext} \n",
    "= \\begin{bmatrix}\n",
    "    R & t \\\\\n",
    "    0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "5. Transformamos los puntos del mundo a la cámara $ \\widetilde{X_c} = M_{ext} * \\widetilde{X_w}  $\n",
    "\n",
    "**B. de la cámara al marco de referencia del mundo**\n",
    "\n",
    "4. Para transformar puntos del sistema de coordenadas de la cámara al sistema de coordenadas del objeto, necesitamos la transformación inversa de $M_{ext}$. Que se calcula como:\n",
    "\n",
    "${M_{ext}} ^{-1} \n",
    "= \\begin{bmatrix}\n",
    "    R^T & -R^T . t \\\\\n",
    "    0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "En donde:\n",
    "- $R^T$ es la inversa de la matriz de rotación R (ya que R es una matriz de ortogonal).\n",
    "- $-R^T . t$ es la traslación inversa.\n",
    "\n",
    "\n",
    "Esta matriz toma un punto 3d en el marco de referencia de la cámara, expresado en coordenadas homogeneas, y lo lleva a un punto en 3d en el marco de referencia del mundo.\n",
    "\n",
    "5. Transformamos los puntos la la cámara al mundo $ \\widetilde{X_w} = {M_{ext}} ^{-1} . \\widetilde{X_c} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a00b52-db8a-40f2-924e-aaf55ec6ea15",
   "metadata": {},
   "source": [
    "### Ejemplos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a7f2cb-b8d9-4a51-bbcc-a4ef101f26c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomemos algunas capturas de ejemplo\n",
    "pose_indices = [3, 16, 23, 24]\n",
    "pose_images = [images[i] for i in pose_indices]\n",
    "show_images(pose_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3754455e-f47b-4005-b6f7-dae6bd9980c6",
   "metadata": {},
   "source": [
    "#### Cómputo de poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157d2a1f-bc2d-4c88-a3fa-abc463abe97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_poses(\n",
    "    images,\n",
    "    camera_calibration,\n",
    "    obj_world_points,\n",
    "    detector,\n",
    "):\n",
    "\n",
    "    K, dist_coeffs = camera_calibration\n",
    "\n",
    "    ret = []\n",
    "    \n",
    "    for image in images:\n",
    "        \n",
    "        found, obj_image_points = detector(image)\n",
    "        if not found:\n",
    "            raise Exception(\"object not detected\")\n",
    "\n",
    "        # compute la pose\n",
    "        pose = cv2.solvePnP(\n",
    "            obj_world_points,\n",
    "            obj_image_points,\n",
    "            K,\n",
    "            dist_coeffs,\n",
    "        )\n",
    "        ret.append(pose)\n",
    "    return ret\n",
    "\n",
    "\n",
    "obj_world_points = checkerboard_world_points\n",
    "camera_calibration = (K, dist_coeffs)\n",
    "detector = lambda image: calib.detect_board(checkerboard, image)\n",
    "\n",
    "# Computa las poses\n",
    "poses = compute_poses(\n",
    "    pose_images,\n",
    "    camera_calibration,\n",
    "    obj_world_points,\n",
    "    detector\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f80c8f8-3e17-4db0-8754-eda5ac61272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "poses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0babf57-883d-46c0-955d-4044ec217d0e",
   "metadata": {},
   "source": [
    "#### A. Del mundo a la cámara\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66708a5e-8011-41f8-9db5-064305e9be09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_homo(points):\n",
    "    h_points = points.reshape(-1, 3)\n",
    "    ones = np.ones( (len(h_points), 1) )\n",
    "    h_points = np.concatenate(\n",
    "        (\n",
    "            h_points, \n",
    "            ones\n",
    "        ), axis=1\n",
    "    )\n",
    "    return h_points\n",
    "    \n",
    "\n",
    "def world_to_camera(\n",
    "    pose,\n",
    "    obj_world_points_h\n",
    "):\n",
    "\n",
    "    ret, rvec, tvec = pose\n",
    "    if not ret:\n",
    "        raise Exception(\"cannot estimate pose\")\n",
    "    \n",
    "    # computamos Rodrigues, para obtener la matriz de rotación\n",
    "    R, _ = cv2.Rodrigues(rvec)\n",
    "    \n",
    "    # armamos la matriz extrínseca:\n",
    "    # Mext T 4x4 que transforma puntos en coordenadas del mundo\n",
    "    # en puntos en coordenadas de la cámara:\n",
    "    # x_c = Mext * x_w \n",
    "    Mext = np.column_stack((R, tvec))\n",
    "    Mext = np.vstack((Mext, [0, 0, 0, 1])) \n",
    "    \n",
    "    # Transformamos puntos en coords del mundo a coords de la cámara\n",
    "    obj_points_cam = (Mext @ obj_world_points_h.T).T\n",
    "    obj_points_cam = obj_points_cam[:, :3] / obj_points_cam[:, 3].reshape(-1, 1)\n",
    "    \n",
    "    return obj_points_cam\n",
    "\n",
    "\n",
    "\n",
    "obj_world_points_h = to_homo(obj_world_points)\n",
    "\n",
    "\n",
    "obj_points_cam = [\n",
    "    world_to_camera(pose, obj_world_points_h)\n",
    "    for pose in poses\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20de8cc3-195b-4e2c-bfc2-ef66ffeb41d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "\n",
    "def linear_cmap(rgb):\n",
    "    \n",
    "    # Normalize the RGB values to the range [0, 1]\n",
    "    \n",
    "    # Create a colormap that transitions from black (0, 0, 0) to the target color\n",
    "    colors = [(0, 0, 0), rgb]  # Start with black, end with target color\n",
    "    cmap = mcolors.LinearSegmentedColormap.from_list(\"custom_cmap\", colors)\n",
    "    return cmap\n",
    "\n",
    "\n",
    "M = 1.0\n",
    "m = 0.0\n",
    "\n",
    "red = (M, m, m)\n",
    "green = (m, M, m)\n",
    "blue = (m, m, M)\n",
    "cyan = (m, M, M)\n",
    "magenta = (M, m, M)\n",
    "yellow = (M, M, m)\n",
    "\n",
    "\n",
    "colors = [red, green, cyan, magenta]\n",
    "cmaps = [\n",
    "    linear_cmap(color)\n",
    "    for color in colors\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003bd54c-ddba-49be-a6fb-8c2cad11913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_line3d(ax, pt1, pt2, color, thickness=None, label=None, style=None):\n",
    "\n",
    "    if thickness is None:\n",
    "        thickness = 3\n",
    "\n",
    "    if style is None:\n",
    "        style = 'solid'\n",
    "    \n",
    "    plt.plot(\n",
    "        [pt1[0], pt2[0]], \n",
    "        [pt1[1], pt2[1]],\n",
    "        [pt1[2], pt2[2]],\n",
    "        color=color,\n",
    "        linewidth=thickness,\n",
    "        label=label,\n",
    "        linestyle=style\n",
    "    )\n",
    "    \n",
    "def plot_axis3d(\n",
    "    ax, \n",
    "    axis, \n",
    "    thickness=None,\n",
    "    style=None,\n",
    "    labels=None,\n",
    "):\n",
    "\n",
    "    origin = axis[0]\n",
    "    red = (1, 0, 0)\n",
    "    green = (0, 1, 0)\n",
    "    blue = (0, 0, 1)\n",
    "\n",
    "    if labels is None:\n",
    "        labels = [\"x\", \"y\", \"z\"]\n",
    "    elif len(labels) < 3:\n",
    "        labels = [None] * 3\n",
    "    plot_line3d(ax, origin, axis[1], red, thickness, labels[0], style)\n",
    "    plot_line3d(ax, origin, axis[2], green, thickness, labels[1], style)\n",
    "    plot_line3d(ax, origin, axis[3], blue, thickness, labels[2], style)\n",
    "\n",
    "\n",
    "def build_axis3d(scale, homo=False, right_handed=True):\n",
    "\n",
    "    axis = np.vstack(\n",
    "        (\n",
    "            np.zeros(3), # origen\n",
    "            np.eye(3) * scale, # crea la identidad 3x3, y multiplica por la escala\n",
    "        )\n",
    "    )\n",
    "    if not right_handed:\n",
    "        axis[3, 2] = -axis[3, 2]\n",
    "\n",
    "    if homo:\n",
    "        axis = to_homo(axis)\n",
    "        axis = axis.T\n",
    "    \n",
    "    return axis\n",
    "\n",
    "def plot_objects_camera(\n",
    "    obj_points_cam,\n",
    "    axis=None,\n",
    "):\n",
    "\n",
    "        \n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    for opc, col in zip(obj_points_cam, colors):\n",
    "        \n",
    "        ax.scatter(*opc.T, marker='o', color=col, label=\"puntos del objeto\")\n",
    "\n",
    "    if axis is not None:\n",
    "        plot_axis3d(ax, axis)\n",
    "    \n",
    "    ax.view_init(elev=10, azim=-130)  # elev=90 para rotar 90 grados sobre el eje X\n",
    "    \n",
    "    #plt.axis('equal')\n",
    "    ax.set_xlim([-500, 500])\n",
    "    ax.set_ylim([-500, 500])\n",
    "    ax.set_zlim([0, 1000])\n",
    "    \n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"Y\")\n",
    "    ax.set_zlabel(\"Z\")\n",
    "    ax.set_title(\"coordenadas de los objetos en el marco de referencia de la cámara\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "axis_len = 6 * square_size_mm\n",
    "axis = build_axis3d(axis_len)\n",
    "\n",
    "plot_objects_camera(\n",
    "    obj_points_cam,\n",
    "    axis=axis\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c4aad3-f5e7-4204-b55b-2af661d361b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(pose_images, colormaps=cmaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe935c57-1dd8-4d51-b9c7-dd15484d67b4",
   "metadata": {},
   "source": [
    "### B. De la cámara al mundo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13691fce-a0df-4db1-aa5e-812031dc7715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def camera_to_world(\n",
    "    pose,\n",
    "    obj_world_points_h\n",
    "):\n",
    "\n",
    "    ret, rvec, tvec = pose\n",
    "    if not ret:\n",
    "        raise Exception(\"cannot estimate pose\")\n",
    "    \n",
    "    # computamos Rodrigues, para obtener la matriz de rotación\n",
    "    R, _ = cv2.Rodrigues(rvec)\n",
    "    \n",
    "    # armamos la matriz extrínseca inversa:\n",
    "    # Mext_inv T 4x4 que transforma puntos en coordenadas de la cámara\n",
    "    # en puntos en coordenadas del mundo:\n",
    "    # x_w = Mext_inv * x_c \n",
    "\n",
    "    R_inv = R.T  # La inversa de R es su transpuesta\n",
    "    t_inv = -R_inv @ tvec\n",
    "    Mext_inv = np.column_stack((R_inv, t_inv))\n",
    "    Mext_inv = np.vstack((Mext_inv, [0, 0, 0, 1]))\n",
    "    \n",
    "    # Transformamos puntos:\n",
    "    ret = (Mext_inv @ obj_world_points_h).T\n",
    "    ret = ret[:, :3]\n",
    "    \n",
    "    return ret\n",
    "\n",
    "\n",
    "def plot_objects_world(\n",
    "    world_object,\n",
    "    obj_points_world\n",
    "):\n",
    "        \n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # grafico el objeto\n",
    "    ax.scatter(*world_object.T, marker='.', color='black', label=\"puntos del objeto\")\n",
    "    \n",
    "    for axis_cam_w, color in zip(obj_points_world, colors):\n",
    "        # graficamos los ejes de cámara en el mundo\n",
    "        pt = axis_cam_w[0]\n",
    "        ax.scatter(*pt.T, marker='o', color=color, s=30, label=\"pose de cámara\")\n",
    "        \n",
    "        plot_axis3d(ax, axis_cam_w, labels=[])\n",
    "    \n",
    "    \n",
    "    # Dibujo los ejes del mundo\n",
    "    axis_w = build_axis3d(500, right_handed=False)\n",
    "    plot_axis3d(ax, axis_w, thickness=1, style=\"dashed\", labels=[\"x world\", \"y world\", \"z world (left handed)\"])\n",
    "    \n",
    "    ax.legend()\n",
    "\n",
    "    ax.view_init(elev=10, azim=-130)  # elev=90 para rotar 90 grados sobre el eje X\n",
    "    \n",
    "    \n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# El objeto del marco de camara a transformar será: \n",
    "# los ejes de coordenadas de la cámara\n",
    "# Que definimos según:\n",
    "axis_len = 6 * square_size_mm\n",
    "axis_cam = build_axis3d(axis_len, homo=True)\n",
    "\n",
    "# para cada pose obtenemos el objeto transformado:\n",
    "obj_points_world = [\n",
    "    camera_to_world(pose, axis_cam)\n",
    "    for pose in poses\n",
    "]\n",
    "\n",
    "plot_objects_world(\n",
    "    checkerboard_world_points,\n",
    "    obj_points_world\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86984439-748f-4849-98e9-de084eceb70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(pose_images, colormaps=cmaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fceedd-5136-440c-8bd9-311baedf4870",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
